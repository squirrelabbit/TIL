

# 1. 회귀

> 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향 이용한 통계학 기법

> 여러개의 독립변수와 한개의 종속변수간의 상관관계 모델링하는 기법 통칭

주어진 피쳐와 결정값 데이터-> 학습->최적 회귀계수찾기!!

# 2. 선형회귀

## 2.1 일반선형회귀

### RSS: 오류값 제곱 더하기

> RSS 최소화하는 W0,W1(회귀계수) 찾기가 머신러닝기반 회귀의 핵심사항

> X,Y가 아닌 W변수 (회귀계수)가 중심변수임을 인지!!!!  이때 X,Y는 상수로 간주

![image-20220202101122646](C:\Users\eunwon\AppData\Roaming\Typora\typora-user-images\image-20220202101122646.png)

### 경사하강법

경사 하강법: 비용 최소화하기-> RSS 편미분

![image-20220203111957595](5. 회귀(regression).assets/image-20220203111957595.png)

![image-20220202101626314](C:\Users\eunwon\AppData\Roaming\Typora\typora-user-images\image-20220202101626314.png)

![image-20220202101649891](C:\Users\eunwon\AppData\Roaming\Typora\typora-user-images\image-20220202101649891.png)

w1 과 w0 를 업데이트하는 함수

```# w1 과 w0 를 업데이트하는 함수
def get_weight_updates(w1, w0, X, y, learning_rate=0.01):
    N = len(y)
    # 먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화
    w1_update = np.zeros_like(w1)
    w0_update = np.zeros_like(w0)
    
    # 예측 배열 계산하고 예측과 실제 값의 차이 계산
    y_pred = np.dot(X, w1.T) + w0
    diff = y - y_pred
         
    # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성
    w0_factors = np.ones((N, 1))

    # w1과 w0을 업데이트할 w1_update와 w0_update 계산
    w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff))
    w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff))    
    
    return w1_update, w0_update
```

반복적으로 w1과 w0를 업데이트 하는 함수
```
def gradient_descent_steps(X, y, iters=10000):
    # w0와 w1을 모두 0으로 초기화. 
    w0 = np.zeros((1, 1))
    w1 = np.zeros((1, 1))
    
    # 인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출하여 
    for ind in range(iters):
        # w1, w0 업데이트 수행. 
        w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)
        w1 = w1 - w1_update
        w0 = w0 - w0_update
              
    return w1, w0  # iter만큼 업데이트된 w1, w0 반환
```
### 오차 함수 정의

```
def get_cost(y, y_pred):
    N = len(y)
    cost = np.sum(np.square(y - y_pred))/N
    return cost
```

### 회귀 평가 지표

![image-20220202102507959](C:\Users\eunwon\AppData\Roaming\Typora\typora-user-images\image-20220202102507959.png)

cross_val_score( )로 MSE 구한 뒤 이를 기반으로 RMSE 구하기
```from sklearn.model_selection import cross_val_score

# features, target 데이터 정의
y_target = bostonDF['PRICE']
X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)

# 선형 회귀 객체 생성
lr = LinearRegression()
lr
```
```# 5 folds 의 개별 Negative MSE scores
neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
neg_mse_scores
```
```o
 # RMSE를 구하기 위해서는 MSE값들에 -1을 곱한 후 평균을 내면 된다.
rmse_scores  = np.sqrt( -1 * neg_mse_scores )
rmse_scores
array([3.52991509, 5.10
 # 5 folds 의 평균 RMSE
avg_rmse = np.mean(rmse_scores)
avg_rmse
 # cross_val_score(scoring="neg_mean_squared_error")로 반환된 값은 모두 음수 
print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))
print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores, 2))
print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))
```

`주의사항` 

> MAE>0
>
> scoring 함수 'neg_mean_absolute_error' 음수값반환-> 작은오류값이 더큰숫자로 인식하게

## 2.2 다항회귀/비선형회귀

다항회귀: 독립변수의 2차 3차방정식과 같은 다항식

비선형 회귀: 회귀 계수가 선형/비선형

```
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

poly = PolynomialFeatures(degree=2)
poly.fit(X)
poly_ftr = poly.transform(X)

 >2차 다항회귀
from sklearn.linear_model import LinearRegression

## Pipeline을 이용하여 PolynomialFeatures 변환과 LinearRegression 적용을 순차적으로 결합. 
p_model = Pipeline([('poly', PolynomialFeatures(degree=2, include_bias=False)),
                    ('linear', LinearRegression())])

# 학습
p_model.fit(X_train, y_train)

# 예측
y_preds = p_model.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
rmse = np.sqrt(mse)

# 평가
print('MSE : {0:.3f} , RMSE : {1:.3F}'.format(mse , rmse))
print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds)))

>3차 다항회귀
from sklearn.linear_model import LinearRegression

## Pipeline을 이용하여 PolynomialFeatures 변환과 LinearRegression 적용을 순차적으로 결합. 
p_model = Pipeline([('poly', PolynomialFeatures(degree=3, include_bias=False)),
                    ('linear', LinearRegression())])

# 학습
p_model.fit(X_train, y_train)

# 예측
y_preds = p_model.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
rmse = np.sqrt(mse)

# 평가
print('MSE : {0:.3f} , RMSE : {1:.3F}'.format(mse , rmse))
print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds)))
```

`다항회귀의 한계` : degree가 높아지면 overfitting이 될 가능성이 커진다.

- degree가 2일 때, RMSE는 4
- degree를 3으로 할 떄, RMSE는 282

## 2.3 overfitting/underfitting

![image-20220202104209319](5. 회귀(regression).assets/image-20220202104209319.png)

![image-20220202104233678](5. 회귀(regression).assets/image-20220202104233678.png)

 - cosine 곡선에 약간의 Noise 변동값을 더하여 실제값 곡선을 만듬

기본 import

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
%matplotlib inline
```

noise집어넣기

```
# X는 0 부터 1까지 30개의 random 값을 순서대로 sampling 한 데이타 입니다.
np.random.seed(0)
n_samples = 30

X = np.sort(np.random.rand(n_samples))

# Cosine 함수
def true_fun(X):
    return np.cos(1.5 * np.pi * X)

# y 값은 cosine 기반의 true_fun() 에서 약간의 Noise 변동값을 더한 값입니다. 
y = true_fun(X) + np.random.randn(n_samples) * 0.1
y
```



# 3. 규제 선형회귀

> 차수(degree)가 높아질수록 회귀계수가 매우크게 설정되고 overfitting된다->예측성능 down
>
> 해결->규제

## 3.1 릿지

L2 규제: 회귀계수 값 크기 감소(not 0) - W제곱에 페널티

![image-20220202105350683](5. 회귀(regression).assets/image-20220202105350683.png)

## 3.2 라쏘

L1 규제:  피처 개수를 줄임(영향력작은회귀계수값을 0으로 변환) - 절대값에 페널티 부여

![image-20220202105406848](5. 회귀(regression).assets/image-20220202105406848.png)

## 3.3 엘라스틱넷 

![image-20220202105548758](5. 회귀(regression).assets/image-20220202105548758.png)



`코드구현`

1.릿지

```
# 앞의 LinearRegression예제에서 분할한 feature 데이터 셋인 X_data과 Target 데이터 셋인 Y_target 데이터셋을 그대로 이용 
from sklearn.linear_model import Ridge
from sklearn.model_selection import cross_val_score

# 릿지 적용된 선형 회귀
ridge = Ridge(alpha = 10)  # alpha 기본이 1

# 평가(RMSE)
neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
rmse_scores  = np.sqrt(-1 * neg_mse_scores)
avg_rmse = np.mean(rmse_scores)
print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3))
print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores,3))
print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse))

앞서 규제를 하지 않았을 때(5.4절)는 RMSE가 5.8 이었는데,
-> 선형 회귀 모델에 릿지 규제를 적용했더니 RMSE가 5.5 로 감소했다.
```

2. 라쏘
```
from sklearn.linear_model import Lasso, ElasticNet

# 회귀 모델의 alpha값에 따른 RMSE를 출력하고, 회귀 계수값들을 DataFrame으로 반환하는 함수
def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True):
    coeff_df = pd.DataFrame()
    if verbose : print('####### ', model_name , '#######')
    
    # 알파값마다 for문을 돌면서 RMSE 계산
    for param in params:
        
        # 입력된 규제 선형회귀('Ridge', 'Lasso', 'ElasticNet') 조건에 맞게 객체 생성
        if model_name =='Ridge': model = Ridge(alpha=param)
        elif model_name =='Lasso': model = Lasso(alpha=param)
        elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7)
            
        # 학습 및 평가 (cross_val_score : MSE를 리스트 형태로 반환해줌 )
        neg_mse_scores = cross_val_score(model, X_data_n,
                                             y_target_n, scoring="neg_mean_squared_error", cv = 5)
        avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores))
        print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} '.format(param, avg_rmse))

        # 모델을 다시 학습하여 변수 별 회귀계수 추출(cross_val_score는 evaluation metric만 반환하므로)
        model.fit(X_data, y_target)
        
        # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. 
        coeff = pd.Series(data=model.coef_ , index=X_data.columns )
        colname='alpha:' + str(param)
        coeff_df[colname] = coeff
    return coeff_df
# end of get_linear_regre_eval

# 회귀 모델의 alpha값에 따른 RMSE를 출력
lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3 ]
coeff_lasso_df = get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)
```

3.엘라스틱

```
# 엘라스틱넷에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출
# l1_ratio는 0.7로 고정
elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3 ]
coeff_elastic_df = get_linear_reg_eval('ElasticNet', params=elastic_alphas,
                                      X_data_n=X_data, y_target_n=y_target)
 -> 알파가 0.5일 때 RMSE가 가장 낮다(베스트 모델)
 
 # 반환된 coeff_elastic_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력
sort_column = 'alpha:' + str(elastic_alphas[0])
coeff_elastic_df.sort_values(by=sort_column, ascending=False)
 
 -> 엘라스틱넷 회귀는 0으로 된 피쳐가 줄었다.
라쏘와 릿지가 적절히 결합된 모델
```



## 3.4  선형회귀모델 성능향상을 위한 피처 엔지니어링

- 타깃값 반드시 정규분포(주로 로그변환)

```python
print(y_target.shape)
plt.hist(y_target, bins=10)
```

![image-20220202110654339](5. 회귀(regression).assets/image-20220202110654339.png)

- 피처값 변환

  1. StandardScaler표준화/MinMaxScaler정규화

  2. Plynomial features 다항특성적용변환

  3. 로그변환
```
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures

# 데이터 정규화(Standard, MinMax, Log)에 따라 데이터를 정규화해주는 함수
# p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음
def get_scaled_data(method = 'None', p_degree=None, input_data=None):
    if method == 'Standard':
        scaled_data = StandardScaler().fit_transform(input_data)
    elif method == 'MinMax':
        scaled_data = MinMaxScaler().fit_transform(input_data)
    elif method == 'Log':
        scaled_data = np.log1p(input_data)
    else:
        scaled_data = input_data

    if p_degree != None:
        scaled_data = PolynomialFeatures(degree=p_degree, 
                                         include_bias=False).fit_transform(scaled_data)
    
    return scaled_data
```
- 데이터변환: 원-핫 인코딩
  

# 4. 로지스틱 회귀

> 선형함수의 회귀최적선 찾는거 X
>
> 시그모이드 함수 최적선 ->반환값을 확률로 간주-> 확률에 따라 분류결정

![image-20220202113906733](5. 회귀(regression).assets/image-20220202113906733.png)

> 주로 이진 분류(0과 1)에 사용, 예측 값: 예측 확률
>
> 예측확률> =0.5 이면 1로 예측

![image-20220202114106271](5. 회귀(regression).assets/image-20220202114106271.png)

![image-20220202114225412](5. 회귀(regression).assets/image-20220202114225412.png)

![image-20220202114314458](5. 회귀(regression).assets/image-20220202114314458.png)

`사이킷런 로지스틱 회귀`

가볍고 빠름 이진분류 예측성능 높음 회소 데이터세트분류good 텍스트분류에 자주사용

LogisticRegression 클래스 ->하이퍼파라미터 penalty(규제유형 기본:l2)

​                                                                           C(규제강도: 알파값역수)-C값이 작을수록 규제강도 up

```from sklearn.metrics import accuracy_score, roc_auc_score

# 학습 (로지스틱 회귀) 
lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)
```

## 4.1 그리드서치로 Logistic regression 하이퍼 파라미터 튜닝
```# penalty : 규제 종류(L1, L2)
# C : 규제 강도 alpha의 역수 (클수록 규제 강도 크다)

from sklearn.model_selection import GridSearchCV

params = {'penalty':['l2', 'l1'],
          'C':[0.01, 0.1, 1, 5, 10]}

grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3 )
grid_clf.fit(data_scaled, cancer.target)
print('최적 하이퍼 파라미터:{}'.format(grid_clf.best_params_))
print('최적 평균 정확도   :{:.3f}'.format(grid_clf.best_score_))     
```


# 5. 회귀 트리

> 결정트리 기반의 앙상블알고리즘 ->분류/회귀 모두 가능

> CART(Classification and Regression Tree) 트리분할알고리즘

- 분류와 유사하게 분할/ 분할기준은 RSS 최소
- 최종분할 후 분할영역 데이터 결정값! 평균값으로 학습/예측

## 5.1 회귀트리프로세스 

여러 트리 알고리즘 비교

![image-20220202115156853](5. 회귀(regression).assets/image-20220202115156853.png)

```
from sklearn.tree import DecisionTreeRegressor       
from sklearn.ensemble import RandomForestRegressor 
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
```

```
# cross_val_score로 교차검증 학습 후 평가지표로 RMSE를 알려주는 함수
def get_model_cv_prediction(model, X_data, y_target):
    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring="neg_mean_squared_error", cv = 5)
    rmse_scores  = np.sqrt(-1 * neg_mse_scores)
    avg_rmse = np.mean(rmse_scores)
    
    print('##### ', model.__class__.__name__ , ' #####')
    print(' 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse), '\n')
```

프로세싱
```
dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)
rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000)
gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)
xgb_reg = XGBRegressor(n_estimators=1000)
lgb_reg = LGBMRegressor(n_estimators=1000)

# 트리 기반의 회귀 모델을 반복하면서 평가 수행 
models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg]

for model in models:  
    get_model_cv_prediction(model, X_data, y_target)
```

## 5.2 트리회귀의 피쳐중요도

```
import seaborn as sns
%matplotlib inline

# 학습 모델
rf_reg = RandomForestRegressor(n_estimators=1000)

# 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다.
rf_reg.fit(X_data, y_target)

# feature_importances_ 메소드로 피처 중요도 확인
feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns )
feature_series
```

```
feature_series = feature_series.sort_values(ascending=False)
sns.barplot(x= feature_series, y=feature_series.index)
```

![image-20220202115453982](5. 회귀(regression).assets/image-20220202115453982.png)

## 5.3 max_depth에 따른 오버피팅



```
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

# 모델 : 선형회귀와 트리회귀 
lr_reg = LinearRegression()
rf_reg2 = DecisionTreeRegressor(max_depth=2)
rf_reg7 = DecisionTreeRegressor(max_depth=7)

# x축 - 테스트 데이터를 4.5 ~ 8.5 범위, 100개 생성. 
X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1)

# 피처는 RM만, 타겟값 PRICE 추출
X_feature = bostonDF_sample['RM'].values.reshape(-1,1)
y_target = bostonDF_sample['PRICE'].values.reshape(-1,1)

# 학습
lr_reg.fit(X_feature, y_target)
rf_reg2.fit(X_feature, y_target)
rf_reg7.fit(X_feature, y_target)

# 예측
pred_lr = lr_reg.predict(X_test)
pred_rf2 = rf_reg2.predict(X_test)
pred_rf7 = rf_reg7.predict(X_test)
```

```
# 선형회귀와 트리회귀의 회귀 예측선 그리기 (X축 값 범위 4.5 ~ 8.5)

fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3)

# 선형회귀
ax1.set_title('Linear Regression')
ax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax1.plot(X_test, pred_lr,label="linear", linewidth=2 )

# 트리회귀 max_depth=2
ax2.set_title('Decision Tree Regression: \n max_depth=2')
ax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax2.plot(X_test, pred_rf2, label="max_depth:3", linewidth=2)

# 트리회귀 max_depth=7 -> overfitting!
ax3.set_title('Decision Tree Regression: \n max_depth=7')
ax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c="darkorange")
ax3.plot(X_test, pred_rf7, label="max_depth:7", linewidth=2)
```

![image-20220202115554394](5. 회귀(regression).assets/image-20220202115554394.png)

`해결방법`

트리의 크기와 노드개수의 제한
