

# 2. 텍스트 전처리

## 1) 토큰화

### 단어토큰화

토큰의 기준을 단어(word)로 하는 경우, 단어 토큰화(word tokenization)

여기서 단어(word)는 단어 단위 외에도 단어구, 의미를 갖는 문자열로도 간주

영어: 띄어쓰기(whitespace)를 기준

한

국어는 띄어쓰기만으로는 단어 토큰을 구분하기 어렵

### 토큰화중 생기는 선택의 순간

```python
from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence
```

![image-20220310155410412](C:\Users\eunwon\AppData\Roaming\Typora\typora-user-images\image-20220310155410412.png)

### 문장토큰화

마침표(.)로 문장을 구분

```python
from nltk.tokenize import sent_tokenize

text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
print('문장 토큰화1 :',sent_tokenize(text))
```

```
문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']
```

문장 중간에 .이 들어가는경우

```python
text = "I am actively looking for Ph.D. students. and you are a Ph.D student."
print('문장 토큰화2 :',sent_tokenize(text))
```

```
문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']
```

#### 한국어

```
pip install ksss
```

```python
import kss

text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'
print('한국어 문장 토큰화 :',kss.split_sentences(text))
```

##### 1) 교착어의 특성
형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위
- 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 

  그 자체로 단어

  체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다. =>**의미있는단어**

- 의존 형태소 : **다른 형태소와 결합하여 사용**되는 형태소. 접사, 어미, 조사, 어간를 말한다.

##### 2) 띄어쓰기 치켜지지않음 

띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있는 언어

### 품사 태깅

단어의 의미를 제대로 파악하기 위해서는 해당 단어가 어떤 품사로 쓰였는지 보는 것이 주요 지표

### 영어 실습

```python
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
tokenized_sentence = word_tokenize(text)

print('단어 토큰화 :',tokenized_sentence)
print('품사 태깅 :',pos_tag(tokenized_sentence))
```

```
단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']
품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]
```

### 한국어 실습

한국어 자연어 처리를 위해서는 **KoNLPy(코엔엘파이)**라는 파이썬 패키지를 사용

 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma).

#### 한국어-okt

```python
from konlpy.tag import Okt
from konlpy.tag import Kkma

okt = Okt()
kkma = Kkma()

print('OKT 형태소 분석 :',okt.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 품사 태깅 :',okt.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 명사 추출 :',okt.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요")) 
```

```
OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']
OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]
OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']
```

\1) morphs : 형태소 추출
\2) pos : 품사 태깅(Part-of-speech tagging)
\3) nouns : 명사 추출

#### 한국어-꼬꼬마

```python
print('꼬꼬마 형태소 분석 :',kkma.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 품사 태깅 :',kkma.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 명사 추출 :',kkma.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))  
```

```
꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']
꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]
꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']
```



## 2) 정제(cleaning) 정규화(nomalizatic)

- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거한다.
- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어준다.

정제 작업은 토큰화 작업보다 앞서 이루어지기도 하지만, 토큰화 작업 이후에도 여전히 남아있는 노이즈들을 제거하기위해 **지속적**으로 이루어지기도 합니다. 사실 완벽한 정제 작업은 어려운 편이라서, 대부분의 경우 이 정도면 됐다.라는 **일종의 합의점**을 찾기도 합니다.

### **1. 규칙에 기반한 표기가 다른 단어들의 통합**

필요에 따라 **직접 코딩을 통해 정의**할 수 있는 정규화 규칙의 예

: 같은 의미를 갖고있음에도, 표기가 다른 단어들을 하나의 단어로 정규화

 표기가 다른 단어들을 통합하는 방법인 어간 추출(stemming)과 표제어 추출(lemmatizaiton)에 대해서 더 자세히 알아봅니다.

### **2. 대, 소문자 통합**

영어권 언어에서 대, 소문자를 통합하는 것은 단어의 개수를 줄일 수 있는 또 다른 정규화 방법입니다. 다.

### **3. 불필요한 단어의 제거**

노이즈 데이터(noise data): 

- 자연어가 아니면서 아무 의미도 갖지 않는 글자들**(특수 문자** 등)
- 분석하고자 하는 **목적에 맞지 않는 불필요 단어**들을 

- 불용어 제거와 등장 빈도가 적은 단어, 길이가 짧은 단어



#### **(1) 등장 빈도가 적은 단어**

#### **(2) 길이가 짧은 단어**

영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 해당됩니다. 

하지만 한국어에서는 길이가 짧은 단어라고 삭제하는 이런 방법이 크게 유효하지 않을 수 있는데 

- 영어 단어의 길이가 한국어 단어의 길이보다는 평균적으로 길다

-> 각 한 글자가 가진 의미의 크기가 다르다/한국어 단어는 한자어가 많고, 한 글자만으로도 이미 의미를 가진 경우가 많다. 

- 영어 길이가1~2인단어 삭제

```
import re
text = "I was wondering if anyone out there could enlighten me on this car."

# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제
shortword = re.compile(r'\W*\b\w{1,2}\b')
print(shortword.sub('', text))
was wondering anyone out there could enlighten this car.
```

### **4. 정규 표현식(Regular Expression)**

길이가 짧은 단어를 제거할 때도, 정규 표현식이 유용하게 사용

## 3) 어간 추출(stemming) 표제어 추출

**정규화 기법** 중 코퍼스에 있는 **단어의 개수를 줄일 수 있는 기법**

- 표제어 추출(lemmatization)과

- 어간 추출(stemming)

서로 다른 단어들이지만, 하나의 단어로 일반화시킬 수 있다면 하나의 단어로 일반화

-> 단어 수를 줄임

  **BoW(Bag of Words)** :단어의 빈도수를 기반으로 문제를 풀고자함표현을 사용하는 자연어 처리 문제에서 주로 사용

자연어 처리에서 전처리, 더 정확히는 정규화의 지향점: 복잡성을 줄이기.

### **1. 표제어 추출(Lemmatization)**

표제어(Lemma)는 한글로는 '표제어' 또는 '기본 사전형 단어' 정도의 의미

표제어 추출은 단어들이 다른 형태를 가지더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단

예am, are, is는 서로 다른 스펠링이지만 뿌리 단어는 be=>표제어:be

형태학적 파싱을 먼저 진행

형태학(morphology)이란 형태소로부터 단어들을 만들어가는 학문

- 형태소의종류

**1) 어간(stem)**
: 단어의 의미를 담고 있는 단어의 **핵심** 부분.

**2) 접사(affix)**
: 단어에 추가적인 의미를 주는 부분.

```
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

print('표제어 추출 전 :',words)
print('표제어 추출 후 :',[lemmatizer.lemmatize(word) for word in words])
표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']
```

어간 추출과는 달리 단어의 형태가 **적절히 보존**되는 양상

하지만 dy나 ha와 같이 의미를 알 수 없는 **적절하지 못한 단어를 출력**

품사 정보를 알아야만 정확한 결과를 얻을 수 있다

- WordNetLemmatizer 사용 

```
lemmatizer.lemmatize('dies', 'v')
'die'
lemmatizer.lemmatize('watched', 'v')
'watch'
lemmatizer.lemmatize('has', 'v')
'have'
```

표제어 추출은 해당 단어의 품사 정보를 보존

어간 추출은  품사 정보가 보존되지 않습니다. 

더 정확히는 어간 추출을 한 결과는 **사전에 존재하지 않는 단어**일 경우가 많습니다.

### **2. 어간 추출(Stemming)**

어간(Stem)을 추출하는 작업

섬세한 작업이 아니기 때문에 어간 추출 후에 나오는 결과 단어는 사전에 존재하지 않는 단어일 수도 있습니다. 

**This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.**

```
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()

sentence = "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
tokenized_sentence = word_tokenize(sentence)

print('어간 추출 전 :', tokenized_sentence)
print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])
어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', "'s", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']
어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', "'s", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']
```

규칙 기반의 접근을 하고 있으므로 어간 추출 후의 결과에는 사전에 없는 단어들도 포함되어 있습니다. 가령, 포터 알고리즘의 어간 추출은 이러한 규칙들을 가집니다.

ALIZE → AL
ANCE → 제거
ICAL → IC

위의 규칙에 따르면 좌측의 단어는 우측의 단어와 같은 결과를 얻게됩니다.

formalize → formal
allowance → allow
electricical → electric

실제 코드를 통해 확인해봅시다.

```
words = ['formalize', 'allowance', 'electricical']

print('어간 추출 전 :',words)
print('어간 추출 후 :',[stemmer.stem(word) for word in words])
어간 추출 전 : ['formalize', 'allowance', 'electricical']
어간 추출 후 : ['formal', 'allow', 'electric']
```

**※ Porter 알고리즘의 상세 규칙은 마틴 포터의 홈페이지에서 확인할 수 있다.**

어간 추출 속도는 표제어 추출보다 일반적으로 빠른데, 

포터 어간 추출기는 **정밀하게 설계되어 정확도가 높으므로** 영어 자연어 처리에서 어간 추출을 하고자 한다면 가장 준수한 선택입니다. 

NLTK: 포터 알고리즘 vs랭커스터 스태머(Lancaster Stemmer) 알고리즘

```
from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer

porter_stemmer = PorterStemmer()
lancaster_stemmer = LancasterStemmer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
print('어간 추출 전 :', words)
print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])
print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])
어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']
랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']
```

동일한 단어에 대해서 표제어 추출과 어간 추출을 각각 수행했을 때, 결과에서 어떤 차이가 있는지 간단한 예를 보겠습니다.

**Stemming**
am → am
the going → the go
having → hav

**Lemmatization**
am → be
the going → the going
having → have

### **3. 한국어에서의 어간 추출**

한국어의 어간: 5언 9품사의 구조

| 언       | 품사               |
| :------- | :----------------- |
| 체언     | 명사, 대명사, 수사 |
| 수식언   | 관형사, 부사       |
| 관계언   | 조사               |
| 독립언   | 감탄사             |
| **용언** | **동사, 형용사**   |

이 중 용언에 해당되는 '동사'와 '형용사'는 어간(stem)과 어미(ending)의 결합

=>기계가 다 알아서해줌

## 4) 불용어

큰 의미가 없는 단어: 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다.

### **1. NLTK에서 불용어 확인하기**

```
stop_words_list = stopwords.words('english')
print('불용어 개수 :', len(stop_words_list))
print('불용어 10개 출력 :',stop_words_list[:10])
불용어 개수 : 179
불용어 10개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]
```

stopwords.words("english")는 NLTK가 정의한 영어 불용어 리스트를 리턴합니다. 출력 결과가 100개 이상이기 때문에 여기서는 간단히 10개만 확인해보았습니다. 'i', 'me', 'my'와 같은 단어들을 NLTK에서 불용어로 정의하고 있음을 확인할 수 있습니다.

### **2. NLTK를 통해서 불용어 제거하기**

```
example = "Family is not an important thing. It's everything."
stop_words = set(stopwords.words('english')) 

word_tokens = word_tokenize(example)

result = []
for word in word_tokens: 
    if word not in stop_words: 
        result.append(word) 

print('불용어 제거 전 :',word_tokens) 
print('불용어 제거 후 :',result)
불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', "'s", 'everything', '.']
```

단어토큰화:word_tokenize

불용어 제거:for문 if not in stop_words

### **3. 한국어에서 불용어 제거하기**

간단하게는 토큰화 후에 조사, 접속사 등을 제거

명사, 형용사와 같은 단어들 중에서 불용어로서 제거하고 싶은 단어들이 생기기도 

결국에는 사용자가 **직접 불용어 사전을 만들게** 되는 경우

```
okt = Okt()

example = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = "를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는"

stop_words = set(stop_words.split(' '))
word_tokens = okt.morphs(example)

result = [word for word in word_tokens if not word in stop_words]

print('불용어 제거 전 :',word_tokens) 
print('불용어 제거 후 :',result)
불용어 제거 전 : ['고기', '를', '아무렇게나', '구', '우려', '고', '하면', '안', '돼', '.', '고기', '라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살', '을', '구울', '때', '는', '중요한', '게', '있지', '.']
불용어 제거 후 : ['고기', '하면', '.', '고기', '라고', '다', '아니거든', '.', '예컨대', '삼겹살', '을', '중요한', '있지', '.']
```

한국어 불용어 리스트

링크 : https://www.ranks.nl/stopwords/korean

## 5) 정규표현식

### **1. 정규 표현식 문법**

정규 표현식을 위해 사용되는 문법 중 특수 문자들은 아래와 같습니다.

| 특수 문자      | 설명                                                         |
| :------------- | :----------------------------------------------------------- |
| .              | 한 개의 임의의 문자를 나타냅니다. (줄바꿈 문자인 \n는 제외)  |
| ?              | 앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있습니다. (문자가 0개 또는 1개) |
| *              | 앞의 문자가 무한개로 존재할 수도 있고, 존재하지 않을 수도 있습니다. (문자가 0개 이상) |
| +              | 앞의 문자가 최소 한 개 이상 존재합니다. (문자가 1개 이상)    |
| ^              | 뒤의 문자열로 문자열이 시작됩니다.                           |
| $              | 앞의 문자열로 문자열이 끝납니다.                             |
| {숫자}         | 숫자만큼 반복합니다.                                         |
| {숫자1, 숫자2} | 숫자1 이상 숫자2 이하만큼 반복합니다. ?, *, +를 이것으로 대체할 수 있습니다. |
| {숫자,}        | 숫자 이상만큼 반복합니다.                                    |
| [ ]            | 대괄호 안의 문자들 중 한 개의 문자와 매치합니다. [amk]라고 한다면 a 또는 m 또는 k 중 하나라도 존재하면 매치를 의미합니다. [a-z]와 같이 범위를 지정할 수도 있습니다. [a-zA-Z]는 알파벳 전체를 의미하는 범위이며, 문자열에 알파벳이 존재하면 매치를 의미합니다. |
| [^문자]        | 해당 문자를 제외한 문자를 매치합니다.                        |
| l              | AlB와 같이 쓰이며 A 또는 B의 의미를 가집니다.                |

정규 표현식 문법에는 역 슬래쉬(\)를 이용하여 자주 쓰이는 문자 규칙들이 있습니다.

| 문자 규칙 | 설명                                                         |
| :-------- | :----------------------------------------------------------- |
| \         | 역 슬래쉬 문자 자체를 의미합니다                             |
| \d        | 모든 숫자를 의미합니다. [0-9]와 의미가 동일합니다.           |
| \D        | 숫자를 제외한 모든 문자를 의미합니다. [^0-9]와 의미가 동일합니다. |
| \s        | 공백을 의미합니다. [ \t\n\r\f\v]와 의미가 동일합니다.        |
| \S        | 공백을 제외한 문자를 의미합니다. [^ \t\n\r\f\v]와 의미가 동일합니다. |
| \w        | 문자 또는 숫자를 의미합니다. [a-zA-Z0-9]와 의미가 동일합니다. |
| \W        | 문자 또는 숫자가 아닌 문자를 의미합니다. [^a-zA-Z0-9]와 의미가 동일합니다. |

### 2. 모듈함수

| 모듈 함수     | 설명                                                         |
| :------------ | :----------------------------------------------------------- |
| re.compile()  | **정규표현식을 컴파일**하는 함수입니다. 다시 말해, **파이썬에게 전해**주는 역할을 합니다. 찾고자 하는 패턴이 빈번한 경우에는 미리 컴파일해놓고 사용하면 속도와 편의성면에서 유리합니다. |
| re.search()   | 문자열 전체에 대해서 정규표현식과 매치되는지를 검색합니다.   |
| re.match()    | 문자열의 처음이 정규표현식과 매치되는지를 검색합니다.        |
| re.split()    | 정규 표현식을 기준으로 문자열을 분리하여 리스트로 리턴합니다. |
| re.findall()  | 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 찾아서 리스트로 리턴합니다. 만약, 매치되는 문자열이 없다면 빈 리스트가 리턴됩니다. |
| re.finditer() | 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열에 대한 이터레이터 객체를 리턴합니다. |
| re.sub()      | 문자열에서 정규 표현식과 일치하는 부분에 대해서 다른 문자열로 대체합니다. |

예

```python
r = re.compile("[a-z]")

# 아무런 결과도 출력되지 않는다.
r.search("AAA")
r.search("111") 
```

```
r.search("aBC")
<_sre.SRE_Match object; span=(0, 1), match='a'>
```



#### **(1) re.match() 와 re.search()의 차이**

search()가 정규 표현식 **전체에 대해서 문자열이 매치**하는지를 본다면, 

match()는 문자열의 **첫 부분부터 정규 표현식과 매치하는지를 확인**합니다. 

```python
r = re.compile("ab.")
r.match("kkkabc") # 아무런 결과도 출력되지 않는다.
r.search("kkkabc")  
<_sre.SRE_Match object; span=(3, 6), match='abc'>   
r.match("abckkk")  
<_sre.SRE_Match object; span=(0, 3), match='abc'>  
```



#### **(2) re.split()**

split() 함수는 입력된 정규 표현식을 기준으로 **문자열들을 분리**하여 리스트로 리턴

토큰화에 유용

- 공백을 기준으로 문자열 분리를 수행하고 결과로서 리스트를 리턴

```
# 공백 기준 분리
text = "사과 딸기 수박 메론 바나나"
re.split(" ", text)
['사과', '딸기', '수박', '메론', '바나나']  
```

- 줄바꿈이나 다른 정규 표현식을 기준으로 텍스트를 분리

```
# 줄바꿈 기준 분리
text = """사과
딸기
수박
메론
바나나"""

re.split("\n", text)
['사과', '딸기', '수박', '메론', '바나나']  
# '+'를 기준으로 분리
text = "사과+딸기+수박+메론+바나나"

re.split("\+", text)
['사과', '딸기', '수박', '메론', '바나나']  
```

#### **(3) re.findall()**

findall() 함수는 정규 표현식과 매치되는 **모든 문자열**들을 리스트로 리턴

단, 매치되는 문자열이 없다면 빈 리스트를 리턴

- 숫자  findall()

```
text = """이름 : 김철수
전화번호 : 010 - 1234 - 1234
나이 : 30
성별 : 남"""

re.findall("\d+", text)
['010', '1234', '1234', '30']
```

하지만 만약 입력 텍스트에 숫자가 없다면 빈 리스트를 리턴하게 됩니다.

```
re.findall("\d+", "문자열입니다.")
[]
```

#### **(4) re.sub()**

sub() 함수는 정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체

특수 문자를 제거하고 싶다면 알파벳 외의 문자는 공백으로 처리하는 등의 용도

```python
text = "Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern."

preprocessed_text = re.sub('[^a-zA-Z]', ' ', text)
print(preprocessed_text)
'Regular expression   A regular expression  regex or regexp     sometimes called a rational expression        is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '  
```

### **3. 정규 표현식 텍스트 전처리 예제**

다음과 같은 임의의 텍스트가 있다고 해봅시다. 테이블 형식의 데이터를 텍스트에 저장하였습니다.

```
text = """100 John    PROF
101 James   STUD
102 Mac   STUD"""
```

s는 공백을 의미하기 때문에 최소 1개 이상의 공백인 패턴

split은 주어진 정규표현식을 기준으로 분리

```
re.split('\s+', text)  
['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']
```

숫자만을

 \d는 숫자에 해당되는 정규표현식입니다. +를 붙이면 최소 1개 이상의 숫자에 해당하는 값을 의미합니다.

 findall()은 해당 표현식에 일치하는 값을 찾아냅니다.

```
re.findall('\d+',text)  
['100', '101', '102]
```

이번에는 텍스트로부터 대문자인 행의 값만 가져와봅시다..

```
re.findall('[A-Z]',text)
['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']
```

대문자가 연속적으로 네 번 등장하는 경우

```
re.findall('[A-Z]{4}',text)  
['PROF', 'STUD', 'STUD']
```

대문자로 구성된 문자열들을 가져옵니다. 이름의 경우에는 대문자와 소문자가 섞여있는 상황입니다. 이름에 대한 행의 값을 갖고오고 싶다면 처음에 대문자가 등장한 후에 소문자가 여러번 등장하는 경우에 매치하게 합니다.

```
re.findall('[A-Z][a-z]+',text)
['John', 'James', 'Mac'] 
```

## 6) 정수 인코딩

자연어 처리에서 텍스트를 숫자로 바꾸는 여러가지 기법

### **1. 정수 인코딩(Integer Encoding)**

어떤 과정으로 단어에 정수 인덱스를 부여하는지

단어를 빈도수 순으로 정렬한 단어 집합(vocabulary)을 만들고, 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여하는 방법

#### **enumerate 이해하기**:각 단어에 인덱스부여

enumerate()는 **순서가 있는 자료형**(list, set, tuple, dictionary, string)을 입력으로 받아 **인덱스를 순차적으로 함께 리턴**한다는 특징

```
test_input = ['a', 'b', 'c', 'd', 'e']
for index, value in enumerate(test_input): # 입력의 순서대로 0부터 인덱스를 부여함.
  print("value : {}, index: {}".format(value, index))
value : a, index: 0
value : b, index: 1
value : c, index: 2
value : d, index: 3
value : e, index: 4
```



### **2. 케라스(Keras)의 텍스트 전처리**

사용 방법과 그 특징

```python
from tensorflow.keras.preprocessing.text import Tokenizer
preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]
```

**단어 토큰화까지 수행된** 앞서 사용한 텍스트 데이터와 동일한 데이터를 사용합니다.

#### Tokenizer().fit_on_texts(문장)

```python
tokenizer = Tokenizer()

# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.
tokenizer.fit_on_texts(preprocessed_sentences) 
```
#### word_index
fit_on_texts는 입력한 텍스트로부터 **단어 빈도수**가 높은 순으로 낮은 정수 인덱스를 부여하는데, 정확히 앞서 설명한 정수 인코딩 작업이 이루어진다고 보면됩니다. 각 단어에 인덱스가 어떻게 부여되었는지를 보려면, **word_index**를 사용합니다.

```
print(tokenizer.word_index)
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}
```
#### word_counts
각 단어가 카운트를 수행하였을 때 몇 개였는지를 보고자 한다면 **word_counts**를 사용

```
print(tokenizer.word_counts)
OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])
```
#### texts_to_sequences()
입력으로 들어온 코퍼스에 대해서 각 단어를 **이미 정해진** 인덱스로 변환합니다.

```
print(tokenizer.texts_to_sequences(preprocessed_sentences))
[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]
```

[barber, person]->[1,5]

#### 빈도수 상위다섯개: Tokenizer(num_words=숫자)

tokenizer = Tokenizer(num_words=숫자)와

```
vocab_size = 5
tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용
tokenizer.fit_on_texts(preprocessed_sentences)
```

**num_words에서 +1**을 더해서 값을 넣어주는 이유: 숫자를 0부터 카운트합니다. 

이유: 패딩(padding)이라는 작업 때문

​          숫자 0도 단어 집합의 크기로 고려해야한다

#### oov_token

- 만약 word_index와 word_counts에서도 지정된 num_words만큼의 단어만 남기고 싶다면 아래의 코드도 방법입니다.

```
tokenizer = Tokenizer()
tokenizer.fit_on_texts(preprocessed_sentences)
vocab_size = 5
words_frequency = [word for word, index in tokenizer.word_index.items() if index >= vocab_size + 1] 

# 인덱스가 5 초과인 단어 제거
for word in words_frequency:
    del tokenizer.word_index[word] # 해당 단어에 대한 인덱스 정보를 삭제
    del tokenizer.word_counts[word] # 해당 단어에 대한 카운트 정보를 삭제

print(tokenizer.word_index)
print(tokenizer.word_counts)
print(tokenizer.texts_to_sequences(preprocessed_sentences))
{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}
OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 4)])
[[1, 5], [1, 5], [1, 3, 5], [2], [2, 4, 3, 2], [3, 2], [1, 4], [1, 4], [1, 4, 2], [3, 2, 1], [1, 3]]
```

케라스 토크나이저는 기본적으로 단어 집합에 없는 단어인 OOV에 대해서는 단어를 정수로 바꾸는 과정에서 **아예 단어를 제거**한다는 특징이 있습니다. 단어 집합에 없는 단어들은 OOV로 간주하여 보존하고 싶다면 **Tokenizer의 인자 oov_token**을 사용합니다.

```
# 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2
vocab_size = 5
tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV')
tokenizer.fit_on_texts(preprocessed_sentences)
```

만약 oov_token을 사용하기로 했다면 케라스 토크나이저는 기본적으로 'OOV'의 인덱스를 1로 합니다.

```
print('단어 OOV의 인덱스 : {}'.format(tokenizer.word_index['OOV']))
단어 OOV의 인덱스 : 1
```

이제 코퍼스에 대해서 정수 인코딩을 진행합니다.

```
print(tokenizer.texts_to_sequences(preprocessed_sentences))
[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]
```

빈도수 상위 5개의 단어는 2 ~ 6까지의 인덱스를 가졌으며, 그 외 단어 집합에 없는 'good'과 같은 단어들은 전부 '**OOV'의 인덱스인 1로 인코딩되었습니다.**

## 7) 패딩(Padding)

길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고, 한꺼번에 묶어서 처리

병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업

### 1. Numpy로 패딩하기

```python
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
```

```python
preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]
```

단어 집합을 만들고, 정수 인코딩을 수행합니다.

```python
tokenizer = Tokenizer()
tokenizer.fit_on_texts(preprocessed_sentences)
encoded = tokenizer.texts_to_sequences(preprocessed_sentences)
print(encoded)
```

모든 단어가 고유한 정수로 변환되었습니다.

```
[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]
```

모두 동일한 길이로 맞춰주기 위해서 이 중에서 **가장 길이가 긴 문장의 길이**를 계산해보겠습니다.

```
max_len = max(len(item) for item in encoded)
print('최대 길이 :',max_len)
최대 길이 : 7
```

가장 길이가 긴 문장의 길이는 7입니다. 모든 문장의 길이를 7로 맞춰주겠습니다. 이때 가상의 단어 'PAD'를 사용합니다. **'PAD'라는 단어가 있다고 가정하고, 이 단어는 0번 단어라고 정의**합니다. 길이가 7보다 짧은 문장에는 숫자 **0을 채워서 길이 7로 맞춰줍**니다.

```python
for sentence in encoded:
    while len(sentence) < max_len:
        sentence.append(0)

padded_np = np.array(encoded)
padded_np
array([[ 1,  5,  0,  0,  0,  0,  0],
       [ 1,  8,  5,  0,  0,  0,  0],
       [ 1,  3,  5,  0,  0,  0,  0],
       [ 9,  2,  0,  0,  0,  0,  0],
       [ 2,  4,  3,  2,  0,  0,  0],
       [ 3,  2,  0,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  2,  0,  0,  0,  0],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0,  0,  0]])
```

데이터에 특정 값을 채워서 데이터의 크기(shape)를 조정하는 것을 패딩(padding)이라고 합니다. 숫자 0을 사용하고 있다면 **제로 패딩(zero padding)**이라고 합니다.

### 2. 케라스 전처리 도구로 패딩하기:pad_sequences()

케라스에서는 위와 같은 패딩을 위해 pad_sequences()를 제공하고 있습니다.

```python
from tensorflow.keras.preprocessing.sequence import pad_sequences
```

encoded 값이 위에서 이미 패딩 후의 결과로 저장되었기 때문에 패딩 이전의 값으로 다시 되돌리겠습니다.

```python
encoded = tokenizer.texts_to_sequences(preprocessed_sentences)
print(encoded)
[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]
```

케라스의 pad_sequences를 사용하여 패딩을 해봅시다.

```python
padded = pad_sequences(encoded)
padded
array([[ 0,  0,  0,  0,  0,  1,  5],
       [ 0,  0,  0,  0,  1,  8,  5],
       [ 0,  0,  0,  0,  1,  3,  5],
       [ 0,  0,  0,  0,  0,  9,  2],
       [ 0,  0,  0,  2,  4,  3,  2],
       [ 0,  0,  0,  0,  0,  3,  2],
       [ 0,  0,  0,  0,  1,  4,  6],
       [ 0,  0,  0,  0,  1,  4,  6],
       [ 0,  0,  0,  0,  1,  4,  2],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)
```

Numpy로 패딩을 진행하였을 때와는 패딩 결과가 다른데 그 이유는 pad_sequences는 기본적으로 문서의 뒤에 0을 채우는 것이 아니라 **앞에 0으로 채우기** 때문입니다. 뒤에 0을 채우고 싶다면 인자로 padding='post'를 주면됩니다.

```
padded = pad_sequences(encoded, padding='post')
padded
array([[ 1,  5,  0,  0,  0,  0,  0],
       [ 1,  8,  5,  0,  0,  0,  0],
       [ 1,  3,  5,  0,  0,  0,  0],
       [ 9,  2,  0,  0,  0,  0,  0],
       [ 2,  4,  3,  2,  0,  0,  0],
       [ 3,  2,  0,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  6,  0,  0,  0,  0],
       [ 1,  4,  2,  0,  0,  0,  0],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)
```

Numpy를 이용하여 패딩을 했을 때와 결과가 동일합니다. 실제로 결과가 동일한지 두 결과를 비교합니다.

```
(padded == padded_np).all()
True
```

True값이 리턴됩니다. 두 결과가 동일

- maxlen != max(len(text)) 걍 정수 줌

```
padded = pad_sequences(encoded, padding='post', maxlen=5)
padded
array([[ 1,  5,  0,  0,  0],
       [ 1,  8,  5,  0,  0],
       [ 1,  3,  5,  0,  0],
       [ 9,  2,  0,  0,  0],
       [ 2,  4,  3,  2,  0],
       [ 3,  2,  0,  0,  0],
       [ 1,  4,  6,  0,  0],
       [ 1,  4,  6,  0,  0],
       [ 1,  4,  2,  0,  0],
       [ 3,  2, 10,  1, 11],
       [ 1, 12,  3, 13,  0]], dtype=int32)
```

기존에 5보다 길었다면 데이터가 손실

뒤에서 두번째 문장은 본래 [ 7, 7, 3, 2, 10, 1, 11]였으나 현재는 [ 3, 2, 10, 1, 11]로 변경데이터가 손실될 경우에 앞의 단어가 아니라 **뒤의 단어**가 삭제되도록 하고싶다면 
##### truncating

truncating='post'를 사용할 경우 뒤의 단어가 삭제

```
padded = pad_sequences(encoded, padding='post', truncating='post', maxlen=5)
padded
array([[ 1,  5,  0,  0,  0],
       [ 1,  8,  5,  0,  0],
       [ 1,  3,  5,  0,  0],
       [ 9,  2,  0,  0,  0],
       [ 2,  4,  3,  2,  0],
       [ 3,  2,  0,  0,  0],
       [ 1,  4,  6,  0,  0],
       [ 1,  4,  6,  0,  0],
       [ 1,  4,  2,  0,  0],
       [ 7,  7,  3,  2, 10],
       [ 1, 12,  3, 13,  0]], dtype=int32)
```

##### last value padding

현재 단어가 총 13개이고, 1번부터 13번까지 정수가 사용되었으므로 단어 집합의 크기에 +1을 하면 마지막 숫자인 13보다 1이 큰 14를 얻습니다. 

pad_sequences의 인자로 value를 사용하면 0이 아닌 다른 숫자로 패딩이 가능합니다.

```
padded = pad_sequences(encoded, padding='post', value=last_value)
padded
array([[ 1,  5, 14, 14, 14, 14, 14],
       [ 1,  8,  5, 14, 14, 14, 14],
       [ 1,  3,  5, 14, 14, 14, 14],
       [ 9,  2, 14, 14, 14, 14, 14],
       [ 2,  4,  3,  2, 14, 14, 14],
       [ 3,  2, 14, 14, 14, 14, 14],
       [ 1,  4,  6, 14, 14, 14, 14],
       [ 1,  4,  6, 14, 14, 14, 14],
       [ 1,  4,  2, 14, 14, 14, 14],
       [ 7,  7,  3,  2, 10,  1, 11],
       [ 1, 12,  3, 13, 14, 14, 14]], dtype=int32)
```

## 8) 원-핫 인코딩(One-Hot Encoding)

**단어 집합(vocabulary)** :서로 다른 단어들의 집합

book과 books와 같이 단어의 변형 형태도 다른 단어로 간주

원-핫 인코딩: 

- 먼저 해야할 일은 단어 집합만들기: 

  텍스트의 모든 단어를 중복을 허용하지 않고 모아놓으면 이를 단어 집합

- 단어 집합에 고유한 정수를 부여하는 정수 인코딩을 진행합니다..

-  숫자로 바뀐 단어들을 벡터로 다루고 싶다면 어떻게 하면 될까요?

### **1. 원-핫 인코딩(One-Hot Encoding)이란?**

원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식입니다. 이렇게 표현된 벡터를 원-핫 벡터(One-Hot vector)라고 합니다.

- 첫째, 정수 인코딩을 수행합니다각 단어에 고유한 정수를 부여

- 둘째, 표현하고 싶은 단어의 고유한 정수를 인덱스로 간주하고 

  해당 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여합니다.

**문장 : 나는 자연어 처리를 배운다**

Okt 형태소 분석기를 통해서 문장에 대해서 토큰화를 수행합니다.

```
from konlpy.tag import Okt  

okt = Okt()  
tokens = okt.morphs("나는 자연어 처리를 배운다")  
print(tokens)
['나', '는', '자연어', '처리', '를', '배운다']
```

각 토큰에 대해서 고유한 정수를 부여합니다. 지금은 문장이 짧기 때문에 각 단어의 빈도수를 고려하지 않지만, 빈도수 순으로 단어를 정렬하여 정수를 부여하는 경우가 많습니다.

```python
word_to_index = {word : index for index, word in enumerate(tokens)}
print('단어 집합 :',word_to_index)
단어 집합 : {'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}
```

토큰을 입력하면 해당 토큰에 대한 원-핫 벡터를 만들어내는 함수를 만들었습니다.

```python
def one_hot_encoding(word, word_to_index):
  one_hot_vector = [0]*(len(word_to_index))
  index = word_to_index[word]
  one_hot_vector[index] = 1
  return one_hot_vector
```

'자연어'라는 단어의 원-핫 벡터를 얻어봅시다.

```python
one_hot_encoding("자연어", word_to_index)
[0, 0, 1, 0, 0, 0]  
```

'자연어'는 정수 2이므로 원-핫 벡터는 인덱스 2의 값이 1이며, 나머지 값은 0인 벡터가 나옵니다.

### **2. 케라스(Keras)를 이용한 원-핫 인코딩(One-Hot Encoding)**:to_categorical

keras는 to_categorical()를 지원합니다. 

```
text = "나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야"
```

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical

text = "나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야"

tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
print('단어 집합 :',tokenizer.word_index)
단어 집합 : {'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}
```

위와 같이 생성된 단어 집합(vocabulary)에 있는 단어들로만 구성된 텍스트가 있다면, texts_to_sequences()를 통해서 이를 **정수 시퀀스로 변환가능**합니다. 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트인 sub_text를 만들어 확인해보겠습니다.

```
sub_text = "점심 먹으러 갈래 메뉴는 햄버거 최고야"
encoded = tokenizer.texts_to_sequences([sub_text])[0]
print(encoded)
[2, 5, 1, 6, 3, 7]
```

지금까지 진행한 것은 이미 이전에 정수 인코딩 실습을 하며 배운 내용입니다. 이제 해당 결과를 가지고, 원-핫 인코딩을 진행해보겠습니다. 케라스는 정수 인코딩 된 결과로부터 원-핫 인코딩을 수행하는 to_categorical()를 지원합니다.

```
one_hot = to_categorical(encoded)
print(one_hot)
[[0. 0. 1. 0. 0. 0. 0. 0.] # 인덱스 2의 원-핫 벡터
 [0. 0. 0. 0. 0. 1. 0. 0.] # 인덱스 5의 원-핫 벡터
 [0. 1. 0. 0. 0. 0. 0. 0.] # 인덱스 1의 원-핫 벡터
 [0. 0. 0. 0. 0. 0. 1. 0.] # 인덱스 6의 원-핫 벡터
 [0. 0. 0. 1. 0. 0. 0. 0.] # 인덱스 3의 원-핫 벡터
 [0. 0. 0. 0. 0. 0. 0. 1.]] # 인덱스 7의 원-핫 벡터
```

위의 결과는 "점심 먹으러 갈래 메뉴는 햄버거 최고야"라는 문장이 [2, 5, 1, 6, 3, 7]로 정수 인코딩이 되고나서, 각각의 인코딩 된 결과를 인덱스로 원-핫 인코딩이 수행된 모습을 보여줍니다.

### **3. 원-핫 인코딩(One-Hot Encoding)의 한계**

단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 됨

 가령, 단어가 1,000개인 코퍼스를 가지고 원 핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 됩니다. 다시 말해 모든 단어 각각은 하나의 값만 1을 가지고, 999개의 값은 0의 값을 가지는 벡터가 되는데 이는 저장 공간 측면에서는 **매우 비효율적인 표현 방법**입니다.

또한 원-핫 벡터는 **단어의 유사도를 표현하지 못한다는 단점**이 있습니다. 예를 들어서 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해서 원-핫 인코딩을 해서 각각, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]이라는 원-핫 벡터를 부여받았다고 합시다. 이때 원-핫 벡터로는 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수가 없습니다. 좀 더 극단적으로는 강아지, 개, 냉장고라는 단어가 있을 때 강아지라는 단어가 개와 냉장고라는 단어 중 어떤 단어와 더 유사한지도 알 수 없습니다.

- 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으

  - 첫째는 **카운트 기반**의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있으며, 

  - 둘째는 **예측 기반**으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등
  - 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe

=>워드임베딩/공간의 벡터화

## 9) 데이터의 분리(Splitting Data)

머신 러닝 모델을 학습시키고 평가하기 위해서는 데이터를 적절하게 분리하는 작업이 필요합니다. 이 책에서는 대부분의 경우에서 지도 학습(Supervised Learning)을 다루는데, 이번에는 지도 학습을 위한 데이터 분리 작업에 대해서 배웁니다.

```
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
```

### **1. 지도 학습(Supervised Learning)**

지도학습: '문제'에 해당되는 데이터/ 레이블이라고 부르는 '정답'이 적혀있는 데이터로 구성

문제와 정답을 함께 보면서 열심히 공부-> 향후에 정답이 없는 문제 정답을 예측



**<훈련 데이터>**
X_train : 문제지 데이터
y_train : 문제지에 대한 정답 데이터.

**<테스트 데이터>**
X_test : 시험지 데이터.
y_test : 시험지에 대한 정답 데이터.

학습을 다 한 기계에게 y_test는 보여주지 않고, X_test에 대해서 정답을 예측

기계가 예측한 답과 실제 정답인 y_test를 비교

정답을 얼마나 맞췄는지를 평가=>기계의 정확도(Accuracy)

### **2. X와 y분리하기**

#### **1) zip 함수를 이용하여 분리하기**

zip()함수는 동일한 개수를 가지는 시퀀스 자료형에서 각 순서에 등장하는 원소들끼리 묶어주는 역할을 합니다. 리스트의 리스트 구성에서 zip 함수는 X와 y를 분리하는데 유용합니다. 우선 zip 함수가 어떤 역할을 하는지 확인해보도록 하겠습니다.

```
X, y = zip(['a', 1], ['b', 2], ['c', 3])
print('X 데이터 :',X)
print('y 데이터 :',y)
X 데이터 : ('a', 'b', 'c')
y 데이터 : (1, 2, 3)
```

각 데이터에서 첫번째로 등장한 원소들끼리 묶이고, 두번째로 등장한 원소들끼리 묶인 것을 볼 수 있습니다.

```
# 리스트의 리스트 또는 행렬 또는 뒤에서 배울 개념인 2D 텐서.
sequences = [['a', 1], ['b', 2], ['c', 3]]
X, y = zip(*sequences)
print('X 데이터 :',X)
print('y 데이터 :',y)
X 데이터 : ('a', 'b', 'c')
y 데이터 : (1, 2, 3)
```

각 데이터에서 첫번째로 등장한 원소들끼리 묶이고, 두번째로 등장한 원소들끼리 묶인 것을 볼 수 있습니다. 이를 각각 X데이터와 y데이터로 사용할 수 있습니다.

#### **2) 데이터프레임을 이용하여 분리하기**

```
values = [['당신에게 드리는 마지막 혜택!', 1],
['내일 뵐 수 있을지 확인 부탁드...', 0],
['도연씨. 잘 지내시죠? 오랜만입...', 0],
['(광고) AI로 주가를 예측할 수 있다!', 1]]
columns = ['메일 본문', '스팸 메일 유무']

df = pd.DataFrame(values, columns=columns)
df
```

![img](https://wikidocs.net/images/page/33274/%EB%A9%94%EC%9D%BC.PNG)

데이터프레임은 열의 이름으로 각 열에 접근이 가능하므로, 이를 이용하면 손쉽게 X 데이터와 y 데이터를 분리할 수 있습니다.

```
X = df['메일 본문']
y = df['스팸 메일 유무']
```

X와 y데이터를 출력해보겠습니다.

```
print('X 데이터 :',X.to_list())
print('y 데이터 :',y.to_list())
X 데이터 : ['당신에게 드리는 마지막 혜택!', '내일 뵐 수 있을지 확인 부탁드...', '도연씨. 잘 지내시죠? 오랜만입...', '(광고) AI로 주가를 예측할 수 있다!']
y 데이터 : [1, 0, 0, 1]
```

#### **3) Numpy를 이용하여 분리하기**

임의의 데이터를 만들어서 Numpy의 슬라이싱(slicing)을 사용하여 데이터를 분리해봅시다.

```
np_array = np.arange(0,16).reshape((4,4))
print('전체 데이터 :')
print(np_array)
전체 데이터 :
[[ 0  1  2  3]
 [ 4  5  6  7]
 [ 8  9 10 11]
 [12 13 14 15]]
```

마지막 열을 제외하고 X데이터에 저장합니다. 마지막 열만을 y데이터에 저장합니다.

```
X = np_array[:, :3]
y = np_array[:,3]

print('X 데이터 :')
print(X)
print('y 데이터 :',y)
X 데이터 :
[[ 0  1  2]
 [ 4  5  6]
 [ 8  9 10]
 [12 13 14]]
y 데이터 : [ 3  7 11 15]
```

### **3. 테스트 데이터 분리하기**

이번에는 이미 X와 y가 분리된 데이터에 대해서 테스트 데이터를 분리하는 과정에 대해서 알아보겠습니다.

#### **1) 사이킷 런을 이용하여 분리하기**

사이킷런은 학습용 테스트와 테스트용 데이터를 쉽게 분리할 수 있게 해주는 train_test_split()를 지원합니다.

```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=1234)
```

각 인자는 다음을 의미합니다. train_size와 test_size는 둘 중 하나만 기재해도 됩니다.

X : 독립 변수 데이터. (배열이나 데이터프레임)
y : 종속 변수 데이터. 레이블 데이터.
test_size : 테스트용 데이터 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.
train_size : 학습용 데이터의 개수를 지정한다. 1보다 작은 실수를 기재할 경우, 비율을 나타낸다.
random_state : 난수 시드

예를 들어보겠습니다. 임의로 X 데이터와 y 데이터를 생성했습니다.

```
# 임의로 X와 y 데이터를 생성
X, y = np.arange(10).reshape((5, 2)), range(5)

print('X 전체 데이터 :')
print(X)
print('y 전체 데이터 :')
print(list(y))
X 전체 데이터 :
[[0 1]
 [2 3]
 [4 5]
 [6 7]
 [8 9]]
y 전체 데이터 :
[0, 1, 2, 3, 4]
```

여기서는 7:3의 비율로 데이터를 분리합니다. train_test_split()은 기본적으로 데이터의 순서를 섞고나서 훈련 데이터와 테스트 데이터를 분리합니다. 만약, random_state의 값을 특정 숫자로 기재해준 뒤에 다음에도 동일한 숫자로 기재해주면 항상 동일한 훈련 데이터와 테스트 데이터를 얻을 수 있습니다. 하지만 값을 변경하면 다른 순서로 섞인 채 분리되므로 이전과 다른 훈련 데이터와 테스트 데이터를 얻습니다. 실습을 통해서 이해해봅시다. random_state 값을 임의로 1234로 지정했습니다.

```
# 7:3의 비율로 훈련 데이터와 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)
```

70%의 비율로 분리된 X의 훈련 데이터와 30%의 비율로 분리된 X의 테스트 데이터입니다.

```
print('X 훈련 데이터 :')
print(X_train)
print('X 테스트 데이터 :')
print(X_test)
X 훈련 데이터 :
[[2 3]
 [4 5]
 [6 7]]
X 테스트 데이터 :
[[8 9]
 [0 1]]
```

70%의 비율로 분리된 y의 훈련 데이터와 30%의 비율로 분리된 y의 테스트 데이터입니다.

```
print('y 훈련 데이터 :')
print(y_train)
print('y 테스트 데이터 :')
print(y_test)
y 훈련 데이터 :
[1, 2, 3]
y 테스트 데이터 :
[4, 0]
```

출력 결과를 보면 데이터를 어느 중간 부분에서 앞과 뒤로 자른 것이 아니라 앞에 있던 샘플이 뒤로 가기도하고, 데이터의 순서가 전반적으로 섞이면서 분리된 것을 확인할 수 있습니다. random_state의 의미를 이해하기 위해서 이번에는 random_state의 값을 임의로 다른 값인 1을 주고 다시 분리해보겠습니다. 그리고 y데이터를 출력해봅시다.

```
# random_state의 값을 변경
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

print('y 훈련 데이터 :')
print(y_train)
print('y 테스트 데이터 :')
print(y_test)
y 훈련 데이터 :
[4, 0, 3]
y 테스트 데이터 :
[2, 1]
```

random_state 값이 1234일 때와 전혀 다른 y데이터가 출력됩니다. 데이터가 다른 순서로 섞였다는 의미입니다. 이번에는 다시 random_state의 값을 1234로 주고 다시 y데이터를 출력해봅시다.

```
# random_state을 이전의 값이었던 1234로 변경
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)

print('y 훈련 데이터 :')
print(y_train)
print('y 테스트 데이터 :')
print(y_test)
y 훈련 데이터 :
[1, 2, 3]
y 테스트 데이터 :
[4, 0]
```

이전과 동일한 y데이터가 출력됩니다. random_state의 값을 고정해두면 실행할 때마다 항상 동일한 순서로 데이터를 섞으므로, 동일한 코드를 다음에 재현하고자 할 때 사용할 수 있습니다.

#### **2) 수동으로 분리하기**

데이터를 분리하는 방법 중 하나는 수동으로 분리하는 것입니다. 우선 임의로 X 데이터와 y 데이터를 만들어보겠습니다.

```
# 실습을 위해 임의로 X와 y가 이미 분리 된 데이터를 생성
X, y = np.arange(0,24).reshape((12,2)), range(12)

print('X 전체 데이터 :')
print(X)
print('y 전체 데이터 :')
print(list(y))
X 전체 데이터 :
[[ 0  1]
 [ 2  3]
 [ 4  5]
 [ 6  7]
 [ 8  9]
 [10 11]
 [12 13]
 [14 15]
 [16 17]
 [18 19]
 [20 21]
 [22 23]]
y 전체 데이터 :
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
```

훈련 데이터의 개수와 테스트 데이터의 개수를 정해보겠습니다. num_of_train은 훈련 데이터의 개수를 의미하며, num_of_test는 테스트 데이터의 개수를 의미합니다.

```
num_of_train = int(len(X) * 0.8) # 데이터의 전체 길이의 80%에 해당하는 길이값을 구한다.
num_of_test = int(len(X) - num_of_train) # 전체 길이에서 80%에 해당하는 길이를 뺀다.
print('훈련 데이터의 크기 :',num_of_train)
print('테스트 데이터의 크기 :',num_of_test)
훈련 데이터의 크기 : 9
테스트 데이터의 크기 : 3
```

아직 훈련 데이터와 테스트 데이터를 나눈 것이 아니라 이 두 개의 개수를 몇 개로 할지 정하기만 한 상태입니다. 여기서 num_of_test를 len(X) * 0.2로 계산해서는 안 됩니다. 데이터에 누락이 발생할 수 있습니다. 예를 들어서 전체 데이터의 개수가 4,518이라고 가정했을 때 4,518의 80%의 값은 3,614.4로 소수점을 내리면 3,614가 됩니다. 또한 4,518의 20%의 값은 903.6으로 소수점을 내리면 903이 됩니다. 그리고 3,614 + 903 = 4517이므로 데이터 1개가 누락이 됩니다. 그러므로 어느 한 쪽을 먼저 계산하고 그 값만큼 제외하는 방식으로 계산해야 합니다.

```
X_test = X[num_of_train:] # 전체 데이터 중에서 20%만큼 뒤의 데이터 저장
y_test = y[num_of_train:] # 전체 데이터 중에서 20%만큼 뒤의 데이터 저장
X_train = X[:num_of_train] # 전체 데이터 중에서 80%만큼 앞의 데이터 저장
y_train = y[:num_of_train] # 전체 데이터 중에서 80%만큼 앞의 데이터 저장
```

데이터를 나눌 때는 num_of_train와 같이 하나의 변수만 사용하면 데이터의 누락을 방지할 수 있습니다. 앞에서 구한 데이터의 개수만큼 훈련 데이터와 테스트 데이터를 분할합니다. 그리고 테스트 데이터를 출력하여 정상적으로 분리되었는지 확인합니다.

```
print('X 테스트 데이터 :')
print(X_test)
print('y 테스트 데이터 :')
print(list(y_test))
X 테스트 데이터 :
[[18 19]
 [20 21]
 [22 23]]
y 테스트 데이터 :
[9, 10, 11]
```

각 길이가 3인 것을 확인했습니다. train_test_split()과 다른 점은 데이터가 섞이지 않은 채 어느 지점에서 데이터를 앞과 뒤로 분리했다는 점입니다. 만약, 수동으로 분리하게 된다면 데이터를 분리하기 전에 수동으로 데이터를 섞는 과정이 필요할 수 있습니다. 실제로 뒤에서 이러한 실습들을 진행합니다.

## 10) 한국어 전처리 패키지(Text Preprocessing Tools for Korean Text)

유용한 한국어 전처리 패키지를 정리해봅시다. 앞서 소개한 형태소와 문장 토크나이징 도구들인 KoNLPy와 KSS(Korean Sentence Splitter)와 함께 유용하게 사용할 수 있는 패키지들입니다.

### 1. PyKoSpacing(띄어쓰기)

```
pip install git+https://github.com/haven-jeon/PyKoSpacing.git
```

전희원님이 개발한 PyKoSpacing은 띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지입니다. PyKoSpacing은 대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥 러닝 모델로 준수한 성능을 가지고 있습니다.

```
sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'
```

임의의 문장을 임의로 띄어쓰기가 없는 문장으로 만들었습니다.

```
new_sent = sent.replace(" ", '') # 띄어쓰기가 없는 문장 임의로 만들기
print(new_sent)
김철수는극중두인격의사나이이광수역을맡았다.철수는한국유일의태권도전승자를가리는결전의날을앞두고10년간함께훈련한사형인유연재(김광수분)를찾으러속세로내려온인물이다.
```

이를 PyKoSpacing의 입력으로 사용하여 원 문장과 비교해봅시다.

```
from pykospacing import Spacing
spacing = Spacing()
kospacing_sent = spacing(new_sent) 

print(sent)
print(kospacing_sent)
김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.
김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.
```

정확하게 결과가 일치합니다.

### 2. Py-Hanspell(맞춤법)

```
pip install git+https://github.com/ssut/py-hanspell.git
```

Py-Hanspell은 네이버 한글 맞춤법 검사기를 바탕으로 만들어진 패키지입니다.

```
from hanspell import spell_checker

sent = "맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 "
spelled_sent = spell_checker.check(sent)

hanspell_sent = spelled_sent.checked
print(hanspell_sent)
맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지
```

이 패키지는 띄어쓰기 또한 보정합니다. PyKoSpacing에 사용한 예제를 그대로 사용해봅시다.

```
spelled_sent = spell_checker.check(new_sent)

hanspell_sent = spelled_sent.checked
print(hanspell_sent)
print(kospacing_sent) # 앞서 사용한 kospacing 패키지에서 얻은 결과
김철수는 극 중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연제(김광수 분)를 찾으러 속세로 내려온 인물이다.
김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.
```

PyKoSpacing과 결과가 거의 비슷하지만 조금 다릅니다.

### 4. SOYNLP를 이용한 반복되는 문자 정제

SNS나 채팅 데이터와 같은 한국어 데이터의 경우에는 ㅋㅋ, ㅎㅎ 등의 이모티콘의 경우 불필요하게 연속되는 경우가 많은데 ㅋㅋ, ㅋㅋㅋ, ㅋㅋㅋㅋ와 같은 경우를 모두 서로 다른 단어로 처리하는 것은 불필요합니다. 이에 반복되는 것은 하나로 정규화시켜줍니다.

```
from soynlp.normalizer import *
print(emoticon_normalize('앜ㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠ', num_repeats=2))
print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠ', num_repeats=2))
print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠ', num_repeats=2))
print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ이영화존잼쓰ㅠㅠㅠㅠㅠㅠㅠㅠ', num_repeats=2))
아ㅋㅋ영화존잼쓰ㅠㅠ
아ㅋㅋ영화존잼쓰ㅠㅠ
아ㅋㅋ영화존잼쓰ㅠㅠ
아ㅋㅋ영화존잼쓰ㅠㅠ
```

의미없게 반복되는 것은 비단 이모티콘에 한정되지 않습니다.

```
print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))
print(repeat_normalize('와하하하하하하핫', num_repeats=2))
print(repeat_normalize('와하하하하핫', num_repeats=2))
와하하핫
와하하핫
와하하핫
```

### 5. Customized KoNLPy

영어권 언어는 띄어쓰기만해도 단어들이 잘 분리되지만, 한국어는 그렇지 않다고 앞에서 몇 차례 언급했었습니다. 한국어 데이터를 사용하여 모델을 구현하는 것만큼 이번에는 형태소 분석기를 사용해서 단어 토큰화를 해보겠습니다. 그런데 형태소 분석기를 사용할 때, 이런 상황에 봉착한다면 어떻게 해야할까요?

```
형태소 분석 입력 : '은경이는 사무실로 갔습니다.'
형태소 분석 결과 : ['은', '경이', '는', '사무실', '로', '갔습니다', '.']
```

사실 위 문장에서 '은경이'는 사람 이름이므로 제대로 된 결과를 얻기 위해서는 '은', '경이'와 같이 글자가 분리되는 것이 아니라 '은경이' 또는 최소한 '은경'이라는 단어 토큰을 얻어야만 합니다. 이런 경우에는 형태소 분석기에 사용자 사전을 추가해줄 수 있습니다. '은경이'는 하나의 단어이기 때문에 분리하지말라고 형태소 분석기에 알려주는 것입니다.

사용자 사전을 추가하는 방법은 형태소 분석기마다 다른데, 생각보다 복잡한 경우들이 많습니다. 이번 실습에서는 Customized Konlpy라는 사용자 사전 추가가 매우 쉬운 패키지를 사용합니다.

```
pip install customized_konlpy
```

customized_konlpy에서 제공하는 형태소 분석기 Twitter를 사용하여 앞서 소개했던 예문을 단어 토큰화해봅시다.

```
from ckonlpy.tag import Twitter
twitter = Twitter()
twitter.morphs('은경이는 사무실로 갔습니다.')
['은', '경이', '는', '사무실', '로', '갔습니다', '.']
```

앞서 소개한 예시와 마찬가지로 '은경이'라는 단어가 '은', '경이'와 같이 분리됩니다. 이때, 형태소 분석기 Twitter에 add_dictionary('단어', '품사')와 같은 형식으로 사전 추가를 해줄 수 있습니다.

```
twitter.add_dictionary('은경이', 'Noun')
```

제대로 반영되었는지 동일한 예문을 다시 형태소 분석해봅시다.

```
twitter.morphs('은경이는 사무실로 갔습니다.')
['은경이', '는', '사무실', '로', '갔습니다', '.']
```

'은경이'라는 단어가 제대로 하나의 토큰으로 인식되는 것을 확인할 수 있습니다.
