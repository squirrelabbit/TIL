# 7. 딥러닝

## 2) 인공 신경망

인공 신경망에 대한 기본적인 내용들을 정리합니다.

### **1. 피드 포워드 신경망**

 **(Feed-Forward Neural Network, FFNN)**

![img](https://wikidocs.net/images/page/24987/mlp_final.PNG)

위 그림의 다층 퍼셉트론(MLP)과 같이 오직 입력층에서 출력층 방향으로 연산이 전개되는 신경망을 피드 포워드 신경망(Feed-Forward Neural Network, FFNN)이라고 합니다.

### 2. 전결합층
(Fully-connected layer, FC, Dense layer)

다층 퍼셉트론은 은닉층과 출력층에 있는 모든 뉴런은 바로 **이전 층의 모든 뉴런과 연결**돼 있었습니다. 그와 같이 어떤 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결돼 있는 층을 전결합층(Fully-connected layer) 또는 완전연결층이라고 합니다. 

줄여서 FC라고 부르기도 합니다. 앞서 본 다층 퍼셉트론의 모든 은닉층과 출력층은 전결합층입니다. 동일한 의미로 **밀집층(Dense layer)** 이라고 부르기도 하는데, 케라스에서는 밀집층을 구현할 때 Dense()를 사용합니다.

### **3. 활성화 함수(Activation Function)**

:은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수를 활성화 함수(Activation function)

![img](https://wikidocs.net/images/page/24987/%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0%EA%B3%BC_%ED%99%9C%EC%84%B1%ED%99%94%ED%95%A8%EC%88%98.PNG)

계단 함수(Step function)를 통해 출력값이 0이 될지, 1이 될지를 결정

활성화 함수의 하나의 예제



#### **(1) 활성화 함수의 특징 - 비선형 함수(Nonlinear function)**

활성화 함수의 특징:비선형 함수여야 한다=>직선 1개로는 그릴 수 없는 함수

만약 활성화 함수로 선형 함수를 사용하게 되면 은닉층을 쌓을 수가 없습니다. 

예를 들어 활성화 함수로 선형 함수를 선택하고, 층을 계속 쌓는다고 가정해보겠습니다. 활성화 함수는 f(x)=wx라고 가정합니다. 여기다가 은닉층을 두 개 추가한다고하면 출력층을 포함해서 y(x)=f(f(f(x)))가 됩니다. 이를 식으로 표현하면 w×w×w×x입니다. 그런데 이는 잘 생각해보면 w의 세 제곱값을 k라고 정의해버리면 y(x)=kx와 같이 다시 표현이 가능합니다. 이 경우, 선형 함수로 은닉층을 여러번 추가하더라도 1회 추가한 것과 차이가 없음을 알 수 있습니다.

활성화 함수가 존재하지 않는 선형 함수 층을 사용하지 않는다는 의미는 아닙니다. 종종 활성화 함수를 사용하지 않는 층을 비선형 층들과 함께 인공 신경망의 일부로서 추가하는 경우도 있는데, 학습 가능한 가중치가 새로 생긴다는 점에서 의미가 있습니다. 이와 같이 선형 함수를 사용한 층을 활성화 함수를 사용하는 은닉층과 구분하기 위해서 이 책에서는 선형층(linear layer)이나 투사층(projection layer) 등의 다른 표현을 사용하여 표현합니다. 뒤의 챕터에서 언급할 임베딩 층(embedding layer)도 일종의 선형층입니다. 임베딩 층에는 활성화 함수가 존재하지 않습니다. 활성화 함수를 사용하는 일반적인 은닉층을 선형층과 대비되는 표현을 사용하면 비선형층(nonlinear layer)입니다.

파이썬을 통해 주로 사용되는 활성화 함수를 직접 그려봅시다.

```
import numpy as np
import matplotlib.pyplot as plt
```

#### **(2) 계단 함수(Step function)**

![img](https://wikidocs.net/images/page/24987/step_function.PNG)

```
def step(x):
    return np.array(x > 0, dtype=np.int)
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = step(x)
plt.title('Step Function')
plt.plot(x,y)
plt.show()
```

계단 함수는 거의 사용되지 않지만 퍼셉트론을 통해 인공 신경망을 처음 배울 때 접하게 되는 활성화 함수입니다.

#### **(3) 시그모이드 함수(Sigmoid function)와 기울기 소실** =>ReLU

시그모이드 함수를 사용한 인공 신경망이 있다고 가정해보겠습니다.

![img](https://wikidocs.net/images/page/24987/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C.PNG)

위 인공 신경망의 학습 과정은 다음과 같습니다. 우선 인공 신경망은 입력에 대해서 순전파(forward propagation) 연산을 하고, 그리고 순전파 연산을 통해 나온 예측값과 실제값의 오차를 손실 함수(loss function)을 통해 계산하고, 그리고 이 손실(오차라고도 부릅니다. loss)을 미분을 통해서 기울기(gradient)를 구하고, 이를 통해 출력층에서 입력층 방향으로 가중치와 편향을 업데이트 하는 과정인 역전파(back propagation)를 수행합니다. 역전파에 대해서는 뒤에서 더 자세히 설명하겠지만 일단 여기에서는 인공 신경망에서 출력층에서 입력층 방향으로 가중치와 편향을 업데이트 하는 과정이라고만 언급해두겠습니다. 역전파 과정에서 인공 신경망은 경사 하강법을 사용합니다.

이 시그모이드 함수의 문제점은 미분을 해서 기울기(gradient)를 구할 때 발생합니다.

```
# 시그모이드 함수 그래프를 그리는 코드
def sigmoid(x):
    return 1/(1+np.exp(-x))
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)

plt.plot(x, y)
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()
```

![img](https://wikidocs.net/images/page/60683/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%981.PNG)

위 그래프는 시그모이드 함수의 그래프를 보여줍니다. 시그모이드 함수의 출력값이 0 또는 1에 가까워지면, 그래프의 기울기가 완만해지는 모습을 볼 수 있습니다. 기울기가 완만해지는 구간을 주황색, 그렇지 않은 구간을 초록색으로 칠해보겠습니다.

![img](https://wikidocs.net/images/page/60683/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%982.PNG)

주황색 구간에서는 미분값이 0에 가까운 아주 작은 값입니다. 초록색 구간에서의 미분값은 최대값이 0.25입니다. 다시 말해 시그모이드 함수를 미분한 값은 적어도 0.25 이하의 값입니다. 시그모이드 함수를 활성화 함수로하는 인공 신경망의 층을 쌓는다면, 가중치와 편향을 업데이트 하는 과정인 역전파 과정에서 0에 가까운 값이 누적해서 곱해지게 되면서, 앞단에는 기울기(미분값)가 잘 전달되지 않게 됩니다. 이러한 현상을 **기울기 소실(Vanishing Gradient) 문제**라고 합니다.

시그모이드 함수를 사용하는 은닉층의 개수가 다수가 될 경우에는 0에 가까운 기울기가 계속 곱해지면 앞단에서는 거의 기울기를 전파받을 수 없게 됩니다. 다시 말해 매개변수 w가 업데이트 되지 않아 학습이 되지를 않습니다.

![img](https://wikidocs.net/images/page/60683/%EA%B8%B0%EC%9A%B8%EA%B8%B0_%EC%86%8C%EC%8B%A4.png)

위의 그림은 은닉층이 깊은 신경망에서 기울기 소실 문제로 인해 출력층과 가까운 은닉층에서는 기울기가 잘 전파되지만, 앞단으로 갈수록 기울기가 제대로 전파되지 않는 모습을 보여줍니다. 결론적으로 시그모이드 함수의 은닉층에서의 사용은 지양됩니다. 시그모이드 함수는 주로 이진 분류를 위해 출력층에서 사용합니다.

#### **(4) 하이퍼볼릭탄젠트 함수(Hyperbolic tangent function)**

#### **(5) 렐루 함수(ReLU)**

인공 신경망의 은닉층에서 가장 인기있는 함수입니다. 수식은 f(x)=max(0,x)로 아주 간단합니다.

```
def relu(x):
    return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Relu Function')
plt.show()
```

![img](https://wikidocs.net/images/page/60683/%EB%A0%90%EB%A3%A8%ED%95%A8%EC%88%98.PNG)

렐루 함수는 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환하는 것이 특징인 함수로 출력값이 특정 양수값에 수렴하지 않습니다. 0이상의 입력값의 경우에는 미분값이 항상 1입니다. 깊은 신경망의 은닉층에서 시그모이드 함수보다 훨씬 더 잘 작동합니다. 뿐만 아니라, 렐루 함수는 시그모이드 함수와 하이퍼볼릭탄젠트 함수와 같이 어떤 연산이 필요한 것이 아니라 단순 임계값이므로 연산 속도도 빠릅니다.

하지만 여전히 문제점이 존재하는데, 입력값이 음수면 기울기. 즉, 미분값도 0이 됩니다. 그리고 이 뉴런은 다시 회생하는 것이 매우 어렵습니다. 이 문제를 죽은 렐루(dying ReLU)라고 합니다.

#### **(6) 리키 렐루(Leaky ReLU)**

#### **(7) 소프트맥스 함수(Softamx function)**=>다중분류

은닉층에서는 ReLU(또는 ReLU 변형) 함수들을 사용하는 것이 일반적입니다. 

반면, 소프트맥스 함수는 시그모이드 함수처럼 **출력층**에서 주로 사용됩니다. 

시그모이드 함수가 두 가지 선택지 중 하나를 고르는 이진 분류 (Binary Classification) 문제에 사용된다면 

소프트맥스 함수는 세 가지 이상의 (상호 배타적인) 선택지 중 하나를 고르는 다중 클래스 분류(MultiClass Classification) 문제에 주로 사용됩니다. 

다시 말해서 딥 러닝으로 이진 분류를 할 때는 출력층에 앞서 배운 로지스틱 회귀를 사용하고, 딥 러닝으로 다중 클래스 분류 문제를 풀 때는 출력층에 소프트맥스 회귀를 사용한다고 생각할 수 있습니다.

```
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = np.exp(x) / np.sum(np.exp(x))

plt.plot(x, y)
plt.title('Softmax Function')
plt.show()
```

![img](https://wikidocs.net/images/page/60683/%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4.PNG)

## 3) 행렬곱으로 이해하는 신경망

인공 신경망에서 **입력층에서 출력층 방향**으로 연산을 진행하는 과정을 순전파(Forward Propagation)라고 합니다. 다르게 말하면 주어진 입력이 입력층으로 들어가서 은닉층을 지나 출력층에서 예측값을 얻는 과정을 **순전파**라고 합니다. 

### 1. 순전파(Foward Propagation)

![img](https://wikidocs.net/images/page/36033/%EC%88%9C%EC%A0%84%ED%8C%8C.PNG)

활성화 함수, 은닉층의 수, 각 은닉층의 뉴런 수 등 딥 러닝 모델을 설계하고나면 

입력값은 입력층, 은닉층을 지나면서 각 층에서의 가중치와 함께 연산되며 출력층으로 향함

그리고 출력층에서 모든 연산을 마친 예측값이 나옴



### 2. 행렬곱으로 순전파 이해하기

![img](https://wikidocs.net/images/page/150781/original_nn.PNG)

위와 같은 인공 신경망이 있다고 해봅시다. 입력의 차원이 3, 출력의 차원이 2인 위 인공 신경망을 구현해본다면 다음과 같습니다.

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()

# 3개의 입력과 2개의 출력
model.add(Dense(2, input_dim=3, activation='softmax'))
```

소프트맥스 회귀를 한다고 가정하고 활성화 함수는 소프트맥스 함수를 임의로 기재하였습니다. 인공 신경망이란 표현이 아직 어색한다면 앞에서 배운 소프트맥스 회귀 모델을 만들었다고 생각해도 되겠습니다. 소프트맥스 회귀는 출력 벡터의 차원을 2로 두면 이진 분류를 수행하는 모델이 됩니다. 로지스틱 회귀가 아닌 소프트맥스 회귀로도 이진 분류는 수행 가능함을 기억해둡시다.

케라스에서는 .summary()를 사용하면 해당 모델에 존재하는 모든 매개변수(가중치 w와 편향 b의 개수)를 확인할 수 있습니다.

```
model.summary()
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 2)                 8         
=================================================================
Total params: 8
Trainable params: 8
Non-trainable params: 0
_________________________________________________________________
```

매개변수의 수가 8개라고 나옵니다. 위 신경망에서 학습가능한 매개변수 인 w와 b의 개수가 총 합해서 8개라는 의미입니다. 실제로 그런지 위 신경망을 행렬의 곱셈 관점에서 이해해봅시다.

![img](https://wikidocs.net/images/page/150781/nn.PNG)

위 모델은 입력의 차원이 3, 출력의 차원이 2입니다. 또는 신경망의 용어로서 표현한다면, 입력층의 뉴런이 3개, 출력층의 뉴런이 2개라고 말할 수 있습니다. 위 신경망 그림에서 화살표 각각은 가중치 w를 의미하고 있습니다. 3개의 뉴런과 2개의 뉴런 사이에는 총 6개의 화살표가 존재하는데, 이는 위 신경망에서 가중치 w의 개수가 6개임을 의미합니다.

이를 행렬곱 관점에서는 3차원 벡터에서 2차원 벡터가 되기 위해서 3 × 2 행렬을 곱했다고 이해할 수 있습니다. 그리고 이 행렬 각각의 원소가 각각의 w가 되는 것입니다. 위 그림에서는 y1에 연결되는 화살표 w1, w2, w3를 주황색으로 표현하고, y2에 연결되는 화살표 w4, w5, w6를 초록색으로 표현했습니다.

일반적으로 동그란 뉴런과 화살표로 표현하는 인공 신경망의 그림에서는 편향 b의 경우에는 편의상 생략되는 경우가 많지만, 인공 신경망 내부적으로는 편향 b의 연산 또한 존재합니다. 위 그림에서 뉴런과 화살표로 표현한 인공 신경망의 그림에서는 편향을 표현하지 않았지만, 행렬 연산식에서는 b1과 b2를 표현하였습니다. 편향 b의 개수는 항상 출력의 차원을 기준으로 개수를 확인하면 됩니다. 위의 인공 신경망의 경우에는 출력의 차원이 2인데, 이에 따라서 편향 또한 b1과 b2로 두 개입니다.

가중치 w의 개수가 w1, w2, w3, w4, w5, w6로 총 6개이며 편향 b의 개수가 b1과 b2로 두 개이므로 총 학습가능한 매개변수의 수는 8개입니다. 이는 앞서 model.summary()를 하였을 때 확인한 매개변수의 수인 8개와 일치합니다.

y1과 y2를 구하는 과정을 수식으로 표현한다면 다음과 같이 표현할 수 있습니다.

h1=x1w1+x2w2+x3w3+b1h2=x1w4+x2w5+x3w6+b2[y1,y2]=softmax([h1,h2])

좀 더 간단하게 식을 표현해보겠습니다. 입력 x1, x2, x3을 벡터 X로 명명합니다.
X=[x1,x2,x3]

그리고 w1, w2, w3, w4, w5, w6를 원소로 하는 3 × 2 행렬을 가중치 행렬 W, 그리고 편향 b1 b2를 원소로 하는 벡터를 B, 그리고 y1, y2를 원소로하는 출력 벡터를 Y로 명명합시다. 이 경우, 위의 인공 신경망은 다음과 같이 표현할 수 있습니다.

![img](https://wikidocs.net/images/page/150781/matrix_multiplication.PNG)

다시 말해 수식은 다음과 같습니다.

Y=XW+B

### 3. 행렬곱으로 병렬 연산 이해하기=> 배치연산

인공 신경망을 행렬곱으로 구현할 때의 흥미로운 점 

: 행렬곱을 사용하면 **병렬 연산**(동시에)도 가능

4개의 샘플을 동시에 처리해본다고 가정

4개의 샘플을 하나의 행렬 X로 정의

인공 신경망의 순전파를 행렬곱으로 표현하면 다음과 같습니다.

![img](https://wikidocs.net/images/page/150781/parallel_nn.PNG)

여기서 혼동하지 말아야 할 것은 인공 신경망의 4개의 샘플을 동시에 처리하고 있지만, 여기서 학습가능한 매개변수의 수는 여전히 8개라는 점입니다. 이렇게 인공 신경망이 다수의 샘플을 동시에 처리하는 것을 우리는 '배치 연산'이라고 부릅니다.

난이도를 올려서 중간에 층을 더 추가해봅시다.

### 4. 행렬곱으로 다층 퍼셉트론의 순전파 이해하기

![img](https://wikidocs.net/images/page/24987/neuralnetwork_final.PNG)

위와 같은 인공 신경망이 있다고 합시다. 주어진 인공 신경망을 케라스로 구현해본다면 아래와 같이 구현할 수 있습니다.

#### 1) 코드로 구현하기

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()

# 4개의 입력과 8개의 출력(입력층)
model.add(Dense(8, input_dim=4, activation='relu'))

# 이어서 8개의 출력(은닉층)
model.add(Dense(8, activation='relu'))

# 이어서 3개의 출력(출력층)
model.add(Dense(3, activation='softmax'))
```

위의 코드의 주석에서 () 괄호 안의 값은 각 층에서의 뉴런의 수를 의미하며 입력층부터 출력층까지 순차적으로 인공 신경망의 층을 한 층씩 추가하였습니다. 케라스를 사용하면 이렇게 간단하게 층을 딥하게 쌓은 딥 러닝 모델을 구현할 수 있습니다.



순전파를 진행하고 예측값을 구하고나서 이 다음에 인공 신경망이 해야할 일:

 **예측값과 실제값으로부터 오차를 계산**하고, **오차로부터 가중치와 편향을 업데이트**

:가중치를 업데이트 = 역전파(BackPropagation)

## 04) 딥 러닝의 학습 방법

딥 러닝의 학습 방법의 이해를 위해 필요한 개념인 손실 함수, 옵티마이저, 에포크의 개념에 대해서 정리합니다.

### **1. 손실 함수(Loss function)**

![img](https://wikidocs.net/images/page/36033/%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98.PNG)

손실 함수는 실제값과 예측값의 차이를 수치화해주는 함수입니다. 이 두 값의 차이. 즉, 오차가 클 수록 손실 함수의 값은 크고 오차가 작을 수록 손실 함수의 값은 작아집니다. 회귀에서는 평균 제곱 오차, 분류 문제에서는 크로스 엔트로피를 주로 손실 함수로 사용합니다. 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 w와 편향 b의 값을 찾는 것이 딥 러닝의 학습 과정이므로 손실 함수의 선정은 매우 중요합니다. 앞서 설명했던 손실 함수를 정리해봅시다.

#### **1) MSE(Mean Squared Error, MSE)**

평균 제곱 오차는 **선형 회귀**를 학습할 때 배웠던 손실 함수입니다. 연속형 변수를 예측할 때 사용됩니다.

다음과 같이 compile의 loss에 문자열 'mse'라고 기재하여 사용할 수 있습니다.

```
model.compile(optimizer='adam', loss='mse', metrics=['mse'])
```

compile의 loss는 tf.keras.losses.Loss 인스턴스를 호출하므로 위 코드는 아래와 같이 사용할 수도 있습니다.

```
model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])
```

딥 러닝 자연어 처리는 대부분 분류 문제이므로 평균 제곱 오차보다는 아래의 크로스 엔트로피 함수들을 주로 사용합니다.

#### **2) 이진 크로스 엔트로피(Binary Cross-Entropy)**

이항 교차 엔트로피라고도 부르는 손실 함수입니다. **출력층에서 시그모이드 함수를 사용하는 이진 분류** (Binary Classification)의 경우 binary_crossentropy를 사용합니다. compile의 loss에 문자열로 'binary_crossentropy'를 기재해주면 됩니다. 이는 로지스틱 회귀에서 사용했던 손실 함수입니다.

```
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])
```

compile의 loss는 tf.keras.losses.Loss 인스턴스를 호출하므로 위 코드는 아래와 같이 사용할 수도 있습니다.

```
model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=['acc'])
```

#### **3) 카테고리칼 크로스 엔트로피(Categorical Cross-Entropy)**

**범주형 교차 엔트로피**라고도 부르는 손실 함수입니다. 출력층에서 소프트맥스 함수를 사용하는 다중 클래스 분류(**Multi**-Class Classification)일 경우 categorical_crossentropy를 사용합니다. compile의 loss에 문자열로 'categorical_crossentropy'를 기재해주면 됩니다. 소프트맥스 회귀에서 사용했던 손실 함수입니다.

```
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
```

compile의 loss는 tf.keras.losses.Loss 인스턴스를 호출하므로 위 코드는 아래와 같이 사용할 수도 있습니다.

```
model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer='adam', metrics=['acc'])
```

만약 레이블에 대해서 원-핫 인코딩 과정을 생략하고, 정수값을 가진 레이블에 대해서 다중 클래스 분류를 수행하고 싶다면 다음과 같이 'sparse_categorical_crossentropy'를 사용합니다.

```
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])
```

위 코드는 아래와 같이 사용할 수도 있습니다.

```
model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['acc'])
```

#### **4) 그 외에 다양한 손실 함수들**

아래의 텐서플로우 공식 문서 링크에서 방금 언급하지 않은 손실 함수 외에도 다양한 손실 함수들을 확인할 수 있습니다.

https://www.tensorflow.org/api_docs/python/tf/keras/losses

지금까지 자주 사용하는 손실 함수 몇 가지에 대해서 정리해봤습니다. 위 compile 코드에서 optimizer='adam' 이라는 부분에 주목해봅시다. 이는 아담이라는 옵티마이저를 사용했다라는 의미입니다. 손실 함수의 선정만큼이나 옵티마이저의 선정 또한 중요합니다. 이어서 옵티마이저에 대해서 정리해봅시다.

### **2. 배치 크기(Batch Size)에 따른 경사 하강법**

![img](https://wikidocs.net/images/page/36033/%EC%97%AD%EC%A0%84%ED%8C%8C_%EA%B3%BC%EC%A0%95.PNG)

손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라집니다.

 **배치(Batch)**:가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양

#### **1) 배치 경사 하강법(Batch Gradient Descent)**

배치 경사 하강법(Batch Gradient Descent)은 가장 기본적인 경사 하강법입니다. 배치 경사 하강법은 옵티마이저 중 하나로 오차(loss)를 구할 때 전체 데이터를 고려합니다. 딥 러닝에서는 전체 데이터에 대한 한 번의 훈련 횟수를 1 에포크라고 하는데, 배치 경사 하강법은 한 번의 에포크에 모든 매개변수 업데이트를 단 한 번 수행합니다. 배치 경사 하강법은 전체 데이터를 고려해서 학습하므로 한 번의 매개 변수 업데이트에 시간이 오래 걸리며, 메모리를 크게 요구한다는 단점이 있습니다.

```
model.fit(X_train, y_train, batch_size=len(X_train))
```

#### **2) 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent, SGD)**

기존의 배치 경사 하강법은 전체 데이터에 대해서 계산을 하다보니 시간이 너무 오래걸린다는 단점이 있습니다. 배치 크기가 1인 확률적 경사 하강법은 매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법입니다. 더 적은 데이터를 사용하므로 더 빠르게 계산할 수 있습니다.

![img](https://wikidocs.net/images/page/24987/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95SGD.PNG)

위 그림에서 좌측은 배치 경사 하강법, 우측은 배치 크기가 1인 확률적 경사 하강법이 최적해를 찾아가는 모습을 보여주고 있습니다. 확률적 경사 하강법은 매개변수의 변경폭이 불안정하고, 때로는 배치 경사 하강법보다 정확도가 낮을 수도 있지만 하나의 데이터에 대해서만 메모리에 저장하면 되므로 자원이 적은 컴퓨터에서도 쉽게 사용가능 하다는 장점이 있습니다. 케라스에서는 아래와 같이 사용합니다.

```
model.fit(X_train, y_train, batch_size=1)
```

#### **3) 미니 배치 경사 하강법(Mini-Batch Gradient Descent)**

전체 데이터도, 1개의 데이터도 아닐 때, 배치 크기를 지정하여 해당 데이터 개수만큼에 대해서 계산하여 매개 변수의 값을 조정하는 경사 하강법을 미니 배치 경사 하강법

미니 배치 경사 하강법은 전체 데이터를 계산하는 것보다 **빠르며, SGD보다 안정적**이라는 장점이 있습니다. 

가장 많이 사용되는 경사 하강법으로 주로 배치 크기를 지정하여 미니 배치 경사 하강법으로 학습

```
model.fit(X_train, y_train, batch_size=128)
```

model.fit()에서 배치 크기를 별도로 지정해주지 않을 경우에 기본값은 2의 5제곱에 해당하는 숫자인 32로 설정

### 3. 옵티마이저(Optimizer)

#### **1) 모멘텀(Momentum)**

모멘텀(Momentum)은 관성이라는 물리학의 법칙을 응용한 방법입니다. 모멘텀 경사 하강법에 관성을 더 해줍니다. 모멘텀은 경사 하강법에서 계산된 접선의 기울기에 한 시점 전의 접선의 기울기값을 일정한 비율만큼 반영합니다. 이렇게 하면 마치 언덕에서 공이 내려올 때, 중간에 작은 웅덩이에 빠지더라도 관성의 힘으로 넘어서는 효과를 줄 수 있습니다.

![img](https://wikidocs.net/images/page/24987/%EB%A1%9C%EC%BB%AC%EB%AF%B8%EB%8B%88%EB%A9%88.PNG)

전체 함수에 걸쳐 최소값을 **글로벌 미니멈(Global Minimum)** 이라고 하고, 글로벌 미니멈이 아닌 특정 구역에서의 최소값인 **로컬 미니멈(Local Minimum)** 이라고 합니다. 로컬 미니멈에 도달하였을 때 글로벌 미니멈으로 잘못 인식하여 탈출하지 못하였을 상황에서 모멘텀. 즉, 관성의 힘을 빌리면 값이 조절되면서 현재의 로컬 미니멈에서 탈출하고 글로벌 미니멈 내지는 더 낮은 로컬 미니멈으로 갈 수 있는 효과를 얻을 수도 있습니다.

```
tf.keras.optimizers.SGD(lr=0.01, momentum=0.9)
```

#### **2) 아다그라드(Adagrad)**

매개변수들은 각자 의미하는 바가 다른데, 모든 매개변수에 동일한 학습률(learning rate)을 적용하는 것은 비효율적입니다. 아다그라드는 각 매개변수에 서로 다른 학습률을 적용시킵니다. 이때 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정시킵니다.

```
tf.keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)
```

#### **3) 알엠에스프롭(RMSprop)**

아다그라드는 학습을 계속 진행한 경우에는, 나중에 가서는 학습률이 지나치게 떨어진다는 단점이 있는데 이를 다른 수식으로 대체하여 이러한 단점을 개선하였습니다.

```
tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)
```

#### **4) 아담(Adam)**

아담은 알엠에스프롭과 모멘텀 두 가지를 합친 듯한 방법으로, 방향과 학습률 두 가지를 모두 잡기 위한 방법입니다.

```
tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
```

#### 5) 사용 방법

각 옵티마이저 인스턴스는 compile의 optimizer에서 호출합니다. 예를 들어 아담(adam)은 다음과 같이 코드를 작성합니다.

```
adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])
```

하지만 다음과 같이 단순히 문자열로 'adam'으로 작성하더라도 동작합니다.

```
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
```

다른 옵티마이저들도 마찬가지입니다. optimizer='sgd', optimizer='rmsprop'와 같이 각 옵티마이저를 문자열로 호출할 수 있습니다. 케라스의 옵티마이저 사용법은 아래의 링크에서 좀 더 상세히 확인할 수 있습니다.

링크 : https://www.tensorflow.org/api_docs/python/tf/keras/optimizers

### **4. 역전파(BackPropagation)**



### **5. 에포크와 배치 크기와 이터레이션(Epochs and Batch size and Iteration)**

기계는 실제값과 예측값의 오차로부터 옵티마이저를 통해서 가중치를 업데이트=> **학습**

![img](https://wikidocs.net/images/page/36033/batchandepochiteration.PNG)

위 그림은 에포크와 배치 크기와 이터레이션의 차이를 보여줍니다.

1iteration=>가중치업데이트

#### **1) 에포크(Epoch)**

에포크란 인공 신경망에서 전체 데이터에 대해서 **순전파와 역전파가 끝난 상태**를 말합니다. 

만약 에포크가 50이라고 하면, 전체 데이터 단위로는 총 50번 학습합니다. 문제지에 비유하면 문제지를 50번 푼 셈입니다. 

이 에포크 횟수가 지나치거나 너무 적으면 앞서 배운 과적합과 과소적합이 발생

#### **2) 배치 크기(Batch size)**

배치 크기는 몇 개의 데이터 단위로 매개변수를 업데이트 하는지

여기서 주의할 점은 배치 크기와 배치의 수는 다른 개념이라는 점입니다. 전체 데이터가 2,000일때 배치 크기를 200으로 준다면 배치의 수는 10입니다. 이는 에포크에서 배치 크기를 나눠준 값(2,000/200 = 10)이기도 합니다. 이때 배치의 수를 **이터레이션**이라고 합니다.

#### **3) 이터레이션(Iteration) 또는 스텝(Step)**

이터레이션이란 **한 번의 에포크를 끝내기 위해서 필요한 배치의 수**를 말합니다. 또는 한 번의 에포크 내에서 **이루어지는 매개변수의 업데이트 횟수**이기도 합니다. 

## 4-3) 역전파(BackPropagation) 이해하기

인공 신경망이 순전파 과정을 진행하여 예측값과 실제값의 오차를 계산하였을 때 어떻게 역전파 과정에서 경사 하강법을 사용하여 가중치를 업데이트하는지 직접 계산을 통해 이해해봅시다.

### 1. 인공 신경망의 이해(Neural Network Overview)

여기서 사용할 인공 신경망은 입력층, 은닉층, 출력층 이렇게 3개의 층을 가집니다. 또한 해당 인공 신경망은 두 개의 입력과, 두 개의 은닉층 뉴런, 두 개의 출력층 뉴런을 사용합니다. 은닉층과 출력층의 모든 뉴런은 활성화 함수로 시그모이드 함수를 사용합니다.

![img](https://wikidocs.net/images/page/37406/backpropagation_1.PNG)

위의 그림은 여기서 사용할 인공 신경망의 모습을 보여줍니다. 은닉층과 출력층의 모든 뉴런에서 변수 z가 존재하는데 여기서 **변수 z**는 이전층의 모든 입력이 각각의 가중치와 곱해진 값들이 모두 더해진 가**중합을 의미**합니다. 
이 값은 뉴런에서 아직 시그모이드 함수를 거치지 않은 상태입니다. 즉, 활성화 함수의 입력을 의미합니다. z 우측의 |를 지나서 존재하는 **변수 h 또는 o는 z가 시그모이드 함수를 지난 후의 값으로 각 뉴런의 출력값**을 의미합니다. 
이번 역전파 예제에서는 인공 신경망에 존재하는 모든 가중치 w에 대해서 역전파를 통해 업데이트하는 것을 목표로합니다. 해당 인공 신경망은 편향 b는 고려하지 않습니다.

## **2. 순전파(Forward Propagation)**

![img](https://wikidocs.net/images/page/37406/backpropagation_2.PNG)

주어진 값이 위의 그림과 같을 때 순전파를 진행해봅시다. 위의 그림에서 소수점 앞의 0은 생략하였습니다. 예를 들어 .25는 0.25를 의미합니다. 파란색 숫자는 입력값을 의미하며, 빨간색 숫자는 각 가중치의 값을 의미합니다. 앞으로 진행하는 계산의 결과값은 소수점 아래 여덟번째 자리까지 반올림하여 표기합니다.

각 입력은 입력층에서 은닉층 방향으로 향하면서 각 입력에 해당하는 가중치와 곱해지고, 결과적으로 가중합으로 계산되어 은닉층 뉴런의 시그모이드 함수의 입력값이 됩니다. z1과 z2는 시그모이드 함수의 입력으로 사용되는 각각의 값에 해당됩니다.
z1=w1x1+w2x2=0.3×0.1+0.25×0.2=0.08z2=w3x1+w4x2=0.4×0.1+0.35×0.2=0.11z1과 z2는 각각의 은닉층 뉴런에서 시그모이드 함수를 지나게 되는데 시그모이드 함수가 리턴하는 결과값은 은닉층 뉴런의 최종 출력값입니다. 식에서는 각각 h1과 h2에 해당되며, 아래의 결과와 같습니다.h1=sigmoid(z1)=0.51998934h2=sigmoid(z2)=0.52747230h1과 h2 이 두 값은 다시 출력층의 뉴런으로 향하게 되는데 이때 다시 각각의 값에 해당되는 가중치와 곱해지고, 다시 가중합 되어 출력층 뉴런의 시그모이드 함수의 입력값이 됩니다. 식에서는 각각 z3과 z4에 해당됩니다.z3=w5h1+w6h2=0.45×h1+0.4×h2=0.44498412z4=w7h1+w8h2=0.7×h1+0.6×h2=0.68047592z3과 z4이 출력층 뉴런에서 시그모이드 함수를 지난 값은 이 인공 신경망이 최종적으로 계산한 출력값입니다. 실제값을 예측하기 위한 값으로서 예측값이라고도 부릅니다.o1=sigmoid(z3)=0.60944600o2=sigmoid(z4)=0.66384491이제 해야할 일은 예측값과 실제값의 오차를 계산하기 위한 오차 함수를 선택하는 것입니다. 오차(Error)를 계산하기 위한 손실 함수(Loss function)로는 평균 제곱 오차 MSE를 사용합니다. 식에서는 실제값을 target이라고 표현하였으며, 순전파를 통해 나온 예측값을 output으로 표현하였습니다. 그리고 각 오차를 모두 더하면 전체 오차 Etotal가 됩니다.Eo1=12(targeto1−outputo1)2=0.02193381Eo2=12(targeto2−outputo2)2=0.00203809Etotal=Eo1+Eo2=0.02397190

## **3. 역전파 1단계(BackPropagation Step 1)**

순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트해갑니다. 출력층 바로 이전의 은닉층을 N층이라고 하였을 때, 출력층과 N층 사이의 가중치를 업데이트하는 단계를 역전파 1단계, 그리고 N층과 N층의 이전층 사이의 가중치를 업데이트 하는 단계를 역전파 2단계라고 해봅시다.

![img](https://wikidocs.net/images/page/37406/backpropagation_3.PNG)

역전파 1단계에서 업데이트 해야 할 가중치는 w5,w6,w7,w8 총 4개입니다. 원리 자체는 동일하므로 우선 w5에 대해서 먼저 업데이트를 진행해보겠습니다. 경사 하강법을 수행하려면 가중치 w5를 업데이트 하기 위해서 ∂Etotal∂w5를 계산해야 합니다.

∂Etotal∂w5를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라서 이와 같이 풀어 쓸 수 있습니다.∂Etotal∂w5=∂Etotal∂o1×∂o1∂z3×∂z3∂w5위의 식에서 우변의 세 개의 각 항에 대해서 순서대로 계산해봅시다. 우선 첫번째 항에 대해서 계산해보겠습니다. 미분을 진행하기 전에 Etotal의 값을 상기해봅시다. Etotal은 앞서 순전파를 진행하고 계산했던 전체 오차값입니다. 식은 다음과 같습니다.Etotal=12(targeto1−outputo1)2+12(targeto2−outputo2)2이에 ∂Etotal∂o1는 다음과 같습니다.∂Etotal∂o1=2×12(targeto1−outputo1)2−1×(−1)+0∂Etotal∂o1=−(targeto1−outputo1)=−(0.4−0.60944600)=0.20944600

이제 두번째 항을 주목해봅시다. o1이라는 값은 시그모이드 함수의 출력값입니다. 그런데 시그모이드 함수의 미분은 f(x)×(1−f(x))입니다. 앞으로의 계산 과정에서도 계속해서 시그모이드 함수를 미분해야 하는 상황이 생기므로 기억해둡시다. 이에 따라서 두번째 항의 미분 결과는 다음과 같습니다.
(시그모이드 함수 미분 참고 링크 : https://en.wikipedia.org/wiki/Logistic_function#Derivative)
∂o1∂z3=o1×(1−o1)=0.60944600(1−0.60944600)=0.23802157마지막으로 세번째 항은 h1의 값과 동일합니다.∂z3∂w5=h1=0.51998934우변의 모든 항을 계산하였습니다. 이제 이 값을 모두 곱해주면 됩니다.
∂Etotal∂w5=0.20944600×0.23802157×0.51998934=0.02592286이제 앞서 배웠던 경사 하강법을 통해 가중치를 업데이트 할 때가 왔습니다! 하이퍼파라미터에 해당되는 학습률(learning rate) α는 0.5라고 가정합니다.
w5+=w5−α∂Etotal∂w5=0.45−0.5×0.02592286=0.43703857이와 같은 원리로 w6+, w7+, w8+을 계산할 수 있습니다.
∂Etotal∂w6=∂Etotal∂o1×∂o1∂z3×∂z3∂w6→w6+=0.38685205∂Etotal∂w7=∂Etotal∂o2×∂o2∂z4×∂z4∂w7→w7+=0.69629578∂Etotal∂w8=∂Etotal∂o2×∂o2∂z4×∂z4∂w8→w8+=0.59624247

## **4. 역전파 2단계(BackPropagation Step 2)**

![img](https://wikidocs.net/images/page/37406/backpropagation_4.PNG)

1단계를 완료하였다면 이제 입력층 방향으로 이동하며 다시 계산을 이어갑니다. 위의 그림에서 빨간색 화살표는 순전파의 정반대 방향인 역전파의 방향을 보여줍니다. 현재 인공 신경망은 은닉층이 1개밖에 없으므로 이번 단계가 마지막 단계입니다. 하지만 은닉층이 더 많은 경우라면 입력층 방향으로 한 단계씩 계속해서 계산해가야 합니다.

이번 단계에서 계산할 가중치는 w1,w2,w3,w4입니다. 원리 자체는 동일하므로 우선 w1에 대해서 먼저 업데이트를 진행해보겠습니다. 경사 하강법을 수행하려면 가중치 w1를 업데이트 하기 위해서 ∂Etotal∂w1를 계산해야 합니다.

∂Etotal∂w1를 계산하기 위해 미분의 연쇄 법칙(Chain rule)에 따라서 이와 같이 풀어 쓸 수 있습니다.∂Etotal∂w1=∂Etotal∂h1×∂h1∂z1×∂z1∂w1위의 식에서 우변의 첫번째항인 ∂Etotal∂h1는 다음과 같이 다시 식을 풀어서 쓸 수 있습니다.
∂Etotal∂h1=∂Eo1∂h1+∂Eo2∂h1위의 식의 우변의 두 항을 각각 구해봅시다. 우선 첫번째 항 ∂Eo1∂h1에 대해서 항을 분해 및 계산해보겠습니다.

∂Eo1∂h1=∂Eo1∂z3×∂z3∂h1=∂Eo1∂o1×∂o1∂z3×∂z3∂h1=−(targeto1−outputo1)×o1×(1−o1)×w5=0.20944600×0.23802157×0.45=0.02243370이와 같은 원리로 ∂Eo2∂h1 또한 구합니다.∂Eo2∂h1=∂Eo2∂z4×∂z4∂h1=∂Eo2∂o2×∂o2∂z4×∂z4∂h1=0.00997311

∂Etotal∂h1=0.02243370+0.00997311=0.03240681이제 ∂Etotal∂w1를 구하기 위해서 필요한 첫번째 항을 구했습니다. 나머지 두 항에 대해서 구해보도록 하겠습니다.∂h1∂z1=h1×(1−h1)=0.51998934(1−0.51998934)=0.24960043∂z1∂w1=x1=0.1즉, ∂Etotal∂w1는 다음과 같습니다.∂Etotal∂w1=0.03240681×0.24960043×0.1=0.00080888이제 앞서 배웠던 경사 하강법을 통해 가중치를 업데이트 할 수 있습니다.w1+=w1−α∂Etotal∂w1=0.3−0.5×0.00080888=0.29959556이와 같은 원리로 w2+, w3+, w4+을 계산할 수 있습니다.
∂Etotal∂w2=∂Etotal∂h1×∂h1∂z1×∂z1∂w2→w2+=0.24919112∂Etotal∂w3=∂Etotal∂h2×∂h2∂z2×∂z2∂w3→w3+=0.39964496∂Etotal∂w4=∂Etotal∂h2×∂h2∂z2×∂z2∂w4→w4+=0.34928991

## **5. 결과 확인**

![img](https://wikidocs.net/images/page/37406/backpropagation_5.PNG)

업데이트 된 가중치에 대해서 다시 한 번 순전파를 진행하여 오차가 감소하였는지 확인해보겠습니다.
z1=w1x1+w2x2=0.29959556×0.1+0.24919112×0.2=0.07979778z2=w3x1+w4x2=0.39964496×0.1+0.34928991×0.2=0.10982248h1=sigmoid(z1)=0.51993887h2=sigmoid(z2)=0.52742806z3=w5h1+w6h2=0.43703857×h1+0.38685205×h2=0.43126996z4=w7h1+w8h2=0.69629578×h1+0.59624247×h2=0.67650625o1=sigmoid(z3)=0.60617688o2=sigmoid(z4)=0.66295848Eo1=12(targeto1−outputo1)2=0.02125445Eo2=12(targeto2−outputo2)2=0.00198189Etotal=Eo1+Eo2=0.02323634

기존의 전체 오차 Etotal가 0.02397190였으므로 1번의 역전파로 **오차가 감소한 것**을 확인할 수 있습니다. 

인공 신경망의 학습: 오**차를 최소화하는 가중치를 찾는 목적**으로 **순전파와 역전파를 반복**

## 5) 과적합(Overfitting)을 막는 방법들

### 1. 데이터의 양을 늘리기

데이터 증식 또는 증강(Data Augmentation):

데이터의 양이 적을 경우에는 의**도적으로 기존의 데이터를 조금씩 변형하고 추가**하여 데이터의 양을 늘리기

- 이미지의 경우에는 데이터 증식(이미지를 돌리거나 노이즈를 추가하고, 일부분을 수정)

- 텍스트 데이터의 경우:역 후 재번역을 통해 새로운 데이터를 만들어내는 역번역(Back Translation) 

### 2. 모델의 복잡도 줄이기

인공 신경망의 복잡도는 **은닉층(hidden layer)의 수나 매개변수의 수** 등으로 결정됩니다. 과적합 현상이 포착되었을 때, 인공 신경망 모델에 대해서 할 수 있는 한 가지 조치는 인공 신경망의 복잡도를 줄이는 것 입니다.

- **인공 신경망에서는 모델에 있는 매개변수들의 수를 모델의 수용력(capacity)이라고 하기도 합니다.**

### 3. 가중치 규제(Regularization) 적용하기

 복잡한 모델을 좀 더 간단하게 하는 방법으로 가중치 규제(Regularization)

- L1 규제 : 가중치 **w들의 절대값 합계**를 비용 함수에 추가합니다. L1 노름이라고도 합니다.
- L2 규제 : **모든 가중치 w들의 제곱합**을 비용 함수에 추가합니다. L2 노름이라고도 합니다.

L1 규제: 기존의 비용 함수에 모든 가중치에 대해서 λ∣w∣를 더 한 값을 비용 함수

L2 규제: 기존의 비용 함수에 모든 가중치에 대해서 1/2λw2를 더 한 값을 비용 함수로 합니다. λ는 규제의 강도를 정하는 하이퍼파라미터

λ가 크다면 모델이 훈련 데이터에 대해서 적합한 매개 변수를 찾는 것보다 규제를 위해 추가된 항들을 작게 유지하는 것을 우선한다는 의미

이 두 식 모두 비용 함수를 최소화하기 위해서는 가중치 w들의 값이 작아져야 한다는 특징

L1 규제는 어떤 특성들이 모델에 영향을 주고 있는지를 정확히 판단하고자 할 때 유용합니다. 경험적으로는 L2 규제가 더 잘 동작하므로 L2 규제를 더 권장

 L2 규제는 가중치 감쇠(weight decay)라고도 부릅니다.

- 책에 따라서는 Regularization를 정규화로 번역하기도 하지만, 이는 정규화(Normalization)와 혼동될 수 있으므로 규제 또는 정형화라는 번역이 바람직한 것 같습니다.
- 인공 신경망에서 정규화(Normalization)라는 용어가 쓰이는 기법으로는 또 배치 정규화, 층 정규화 등이 있습니다.

### **4. 드롭아웃(Dropout)**

드롭아웃은 학습 과정에서 **신경망의 일부를 사용하지 않는 방법**입니다.

![img](https://wikidocs.net/images/page/61374/%EB%93%9C%EB%A1%AD%EC%95%84%EC%9B%83.PNG)

드롭아웃의 비율을 0.5로 한다면 학습 과정마다 **랜덤으로 절반의 뉴런을 사용하지 않고**, 절반의 뉴런만을 사용합니다.

- 드롭아웃은 **신경망 학습 시에만** 사용하고, 예측 시에는 사용하지 않는 것이 일반적

- 학습 시에 인공 신경망이 특정 뉴런 또는 특정 조합에 너무 의존적이게 되는 것을 방지

- 매번 랜덤 선택으로 뉴런들을 사용하지 않으므로 **서로 다른 신경망들을 앙상블**하여 사용하는 것 같은 효과를 내어 과적합을 방지

케라스에서는 다음과 같은 방법으로 드롭아웃을 모델에 추가할 수 있습니다.

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dropout, Dense

max_words = 10000
num_classes = 46

model = Sequential()
model.add(Dense(256, input_shape=(max_words,), activation='relu'))
model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5)) # 드롭아웃 추가. 비율은 50%
model.add(Dense(num_classes, activation='softmax'))
```

## 6) 기울기 소실(Gradient Vanishing)과 폭주(Exploding)

- 기울기소실:

  깊은 인공 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈 수록 기울기(Gradient)가 점차적으로 작아지는 현상이 발생할 수 있습니다. **입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면** 결국 최적의 모델을 찾을 수 없게 됩니다.

- **기울기 폭주(Gradient Exploding)** :

  기울기가 점차 커지더니 가중치들이 비정상적으로 큰 값이 되면서 결국 발산되기도 합니다. 이순환 신경망(Recurrent Neural Network, RNN)에서 쉽게 발생

### **1. ReLU와 ReLU의 변형들**

앞에서 배운 내용을 간단히 복습해봅시다. 시그모이드 함수를 사용하면 입력의 절대값이 클 경우에 시그모이드 함수의 출력값이 0 또는 1에 수렴하면서 기울기가 0에 가까워집니다. 그래서 역전파 과정에서 전파 시킬 기울기가 점차 사라져서 입력층 방향으로 갈 수록 제대로 역전파가 되지 않는 기울기 소실 문제가 발생할 수 있습니다.

기울기 소실을 완화하는 가장 간단한 방법은 은닉층의 활성화 함수로 시그모이드나 하이퍼볼릭탄젠트 함수 대신에 ReLU나 ReLU의 변형 함수와 같은 Leaky ReLU를 사용하는 것입니다.

- 은닉층에서는 시그모이드 함수를 사용하지 마세요.
- Leaky ReLU를 사용하면 모든 입력값에 대해서 기울기가 0에 수렴하지 않아 죽은 ReLU 문제를 해결합니다.
- 은닉층에서는 ReLU나 Leaky ReLU와 같은 ReLU 함수의 변형들을 사용하세요.

### **2. 그래디언트 클리핑(Gradient Clipping)**

그래디언트 클리핑은 말 그대로 기울기 값을 자르는 것을 의미합니다. 기울기 폭주를 막기 위해 임계값을 넘지 않도록 값을 자릅니다. 다시 말해서 임계치만큼 크기를 감소시킵니다. 이는 뒤에서 배울 신경망인 RNN에서 유용합니다. RNN은 역전파 과정에서 시점을 역행하면서 기울기를 구하는데, 이때 기울기가 너무 커질 수 있기 때문입니다. 케라스에서는 다음과 같은 방법으로 그래디언트 클리핑을 수행합니다.

```
from tensorflow.keras import optimizers

Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)
```

### **3. 가중치 초기화(Weight initialization)**

같은 모델을 훈련시키더라도 가중치가 **초기에 어떤 값을 가졌느냐**에 따라서 모델의 훈련 결과가 달라지기도 합니다. 다시 말해 가중치 초기화만 적절히 해줘도 기울기 소실 문제과 같은 문제를 완화시킬 수 있습니다.

#### **1) 세이비어 초기화(Xavier Initialization)**

논문 : http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf

2010년 세이비어 글로럿과 요슈아 벤지오는 가중치 초기화가 모델에 미치는 영향을 분석하여 새로운 초기화 방법을 제안했습니다. 이 초기화 방법은 제안한 사람의 이름을 따서 **세이비어(Xavier Initialization) 초기화** 또는 글로럿 초기화(Glorot Initialization)라고 합니다.

이 방법은 균등 분포(Uniform Distribution) 또는 정규 분포(Normal distribution)로 초기화 할 때 두 가지 경우로 나뉘며, 이전 층의 뉴런 개수와 다음 층의 뉴런 개수를 가지고 식을 세웁니다. 이전 층의 뉴런의 개수를 nin, 다음 층의 뉴런의 개수를 nout이라고 해봅시다.

글로럿과 벤지오의 논문에서는 균등 분포를 사용하여 가중치를 초기화할 경우 다음과 같은 균등 분포 범위를 사용하라고 합니다.

W∼Uniform(−6nin+nout,+6nin+nout)

다시 말해 6nin+nout를 m이라고 하였을 때, −m과 +m 사이의 균등 분포를 의미합니다.

정규 분포로 초기화할 경우에는 평균이 0이고, 표준 편차 σ가 다음을 만족하도록 합니다.

σ=2nin+nout

세이비어 초기화: 

- 여러 층의 기울기 분산 사이에 균형을 맞춰서 **특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막습니다**. 
- 그런데 세이비어 초기화는 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태인 활성화 함수와 함께 사용할 경우에는 좋은 성능
-  ReLU와 함께 사용할 경우에는 성능이 좋지 않습니다. => **He 초기화(He initialization)**

#### **2) He 초기화(He initialization)**-Relu

논문 : https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf

He 초기화(He initialization)는 세이비어 초기화와 유사하게 **정규 분포와 균등 분포** 두 가지 경우로 나뉩니다. 다만, He 초기화는 세이비어 초기화와 다르게 **다음 층의 뉴런의 수를 반영하지 않습니다**. 전과 같이 이전 층의 뉴런의 개수를 nin이라고 해봅시다.

He 초기화는 균등 분포로 초기화 할 경우에는 다음과 같은 균등 분포 범위를 가지도록 합니다.
W∼Uniform(−6nin, +6nin)

정규 분포로 초기화할 경우에는 표준 편차 σ가 다음을 만족하도록 합니다.

σ=2nin

- 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용할 경우에는 세이비어 초기화 방법이 효율적입니다.
- ReLU 계열 함수를 사용할 경우에는 He 초기화 방법이 효율적입니다.
- ReLU + He 초기화 방법이 좀 더 보편적입니다.

### **4. 배치 정규화(Batch Normalization)**

기울기 소실이나 폭주를 예방하는 또 다른 방법

배치 정규화:

인공 신경망의 **각 층에 들어가는 입력을 평균과 분산으로 정규화**하여 학습을 효율적으로 

#### **1) 내부 공변량 변화(Internal Covariate Shift)**

배치 정규화를 이해하기 위해서는 내부 공변량 변화(Internal Covariate Shift)를 이해할 필요가 있습니다. 내부 공변량 변화란 학습 과정에서 **층 별로 입력 데이터 분포가 달라지는 현상**을 말합니다. 이전 층들의 학습에 의해 이전 층의 가중치 값이 바뀌게 되면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생합니다. 배치 정규화를 제안한 논문에서는 기울기 소실/폭주 등의 딥 러닝 모델의 불안전성이 층마다 입력의 분포가 달라지기 때문이라고 주장합니다. (배치 정규화를 제안한 논문에서는 이렇게 주장했지만, 뒤에 이어서는 이에 대한 반박들이 나오기는 했습니다. 하지만 그 이유가 어찌되었든 배치 정규화가 학습을 돕는다는 것은 명백합니다.)

- 공변량 변화는 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우를 의미합니다.
- 내부 공변량 변화는 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미합니다.

#### **2) 배치 정규화(Batch Normalization)**

배치 정규화(Batch Normalization)는 표현 그대로 한 번에 들어오는 배치 단위로 정규화하는 것을 말합니다. 배치 정규화는 각 층에서 활성화 함수를 통과하기 전에 수행됩니다. 배치 정규화를 요약하면 다음과 같습니다. 입력에 대해 평균을 0으로 만들고, 정규화를 합니다. 그리고 정규화 된 데이터에 대해서 스케일과 시프트를 수행합니다. 이때 두 개의 매개변수 γ와 β를 사용하는데, γ는 스케일을 위해 사용하고, β는 시프트를 하는 것에 사용하며 다음 레이어에 일정한 범위의 값들만 전달되게 합니다.

배치 정규화의 수식은 다음과 같습니다. 아래에서 BN은 배치 정규화를 의미합니다.

Input : 미니 배치 B={x(1),x(2),...,x(m)}
Output : y(i)=BNγ,β(x(i))

미니배치에대한평균계산μB←1m∑i=1mx(i) # 미니 배치에 대한 평균 계산미니배치에대한분산계산σB2←1m∑i=1m(x(i)−μB)2 # 미니 배치에 대한 분산 계산정규화x^(i)←x(i)−μBσB2+ε # 정규화스케일조정γ과시프트β를통한선형연산y(i)←γx^(i)+β=BNγ,β(x(i)) # 스케일 조정(γ)과 시프트(β)를 통한 선형 연산

- m은 미니 배치에 있는 샘플의 수
- μB는 미니 배치 B에 대한 평균.
- σB는 미니 배치 B에 대한 표준편차.
- x^(i)은 평균이 0이고 정규화 된 입력 데이터.
- ε은 σ2가 0일 때, 분모가 0이 되는 것을 막는 작은 양수. 보편적으로 10−5
- γ는 정규화 된 데이터에 대한 스케일 매개변수로 학습 대상
- β는 정규화 된 데이터에 대한 시프트 매개변수로 학습 대상
- y(i)는 스케일과 시프트를 통해 조정한 BN의 최종 결과

배치 정규화는 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화를 합니다.

- 배치 정규화를 사용하면 시그모이드 함수나 하이퍼볼릭탄젠트 함수를 사용하더라도 기울기 소실 문제가 크게 개선됩니다.
- 가중치 초기화에 훨씬 덜 민감해집니다.
- 훨씬 큰 학습률을 사용할 수 있어 학습 속도를 개선시킵니다.
- 미니 배치마다 평균과 표준편차를 계산하여 사용하므로 훈련 데이터에 일종의 잡음 주입의 부수 효과로 과적합을 방지하는 효과도 냅니다. 다시 말해, 마치 드롭아웃과 비슷한 효과를 냅니다. 물론, 드롭 아웃과 함께 사용하는 것이 좋습니다.
- 배치 정규화는 모델을 복잡하게 하며, 추가 계산을 하는 것이므로 테스트 데이터에 대한 예측 시에 실행 시간이 느려집니다. 그래서 서비스 속도를 고려하는 관점에서는 배치 정규화가 꼭 필요한지 고민이 필요합니다.
- 배치 정규화의 효과는 굉장하지만 내부 공변량 변화때문은 아니라는 논문도 있습니다. : https://arxiv.org/pdf/1805.11604.pdf

#### **3) 배치 정규화의 한계**

배치 정규화는 뛰어난 방법이지만 몇 가지 한계가 존재합니다.

##### **1. 미니 배치 크기에 의존적이다.**

배치 정규화는 너무 작은 배치 크기에서는 잘 동작하지 않을 수 있습니다. 단적으로 배치 크기를 1로 하게되면 분산은 0이 됩니다. 작은 미니 배치에서는 배치 정규화의 효과가 극단적으로 작용되어 훈련에 악영향을 줄 수 있습니다. 배치 정규화를 적용할때는 작은 미니 배치보다는 크기가 어느정도 되는 미니 배치에서 하는 것이 좋습니다. 이처럼 배치 정규화는 배치 크기에 의존적인 면이 있습니다.

##### **2. RNN에 적용하기 어렵다.**

뒤에서 배우겠지만, RNN은 각 시점(time step)마다 다른 통계치를 가집니다. 이는 RNN에 배치 정규화를 적용하는 것을 어렵게 만듭니다. RNN에서 배치 정규화를 적용하기 위한 몇 가지 논문이 제시되어 있지만, 여기서는 이를 소개하는 대신 배치 크기에도 의존적이지 않으며, RNN에도 적용하는 것이 수월한 층 정규화(layer normalization)라는 방법을 소개하고자 합니다.

### **5. 층 정규화(Layer Normalization)**

층 정규화를 이해하기에 앞서 배치 정규화를 시각화해보겠습니다. 다음은 m이 3이고, 특성의 수가 4일 때의 배치 정규화를 보여줍니다. 미니 배치란 동일한 특성(feature) 개수들을 가진 다수의 샘플들을 의미함을 상기합시다.

![img](https://wikidocs.net/images/page/61375/%EB%B0%B0%EC%B9%98%EC%A0%95%EA%B7%9C%ED%99%94.PNG)

반면, 층 정규화는 다음과 같습니다.

![img](https://wikidocs.net/images/page/61375/%EC%B8%B5%EC%A0%95%EA%B7%9C%ED%99%94.PNG)