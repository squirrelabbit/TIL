![image-20220203070923183](4. 분류(classification).assets/image-20220203070923183.png)

# 1. 결정트리와 앙상블

> 결정트리: 데이터 스케일링/정규화 영향이 적음
>
> 과적합발생가능성이 있어 예측성능이 저하
>
> (과적합 극복 트리의 크기를 사전에 제한하는 튜닝필요)
>
> -->앙상블에 결합해 예측성능 향상시키는 좋은 `약한` 학습기가 되어줌
>
> GBM/XGBoost/Light GBM

# 2. 결정트리

![image-20220203071420705](4. 분류(classification).assets/image-20220203071420705.png)

데이터에 있는 규칙을 학습을 통해 `자동` 으로 찾아내 분류규칙을만듬(if-else기반규칙)

## 2.1 정보균일도
- 정보이득= 1- entrophy(데이터의 혼잡도)
 -> 정보이득 높은속성 기준 분할

- 지니계수:평등정도
 -> 지니계수 낮은속성기준분할

## 2.2 규칙노드생성프로세스

![image-20220203072242293](4. 분류(classification).assets/image-20220203072242293.png)

## 2.3 결정 트리 주요 하이퍼 파라미터

![image-20220203072719542](4. 분류(classification).assets/image-20220203072719542.png)

- 과적합 제어

min_samples_split:노드 분할 최소한의 샘플데이터수 =>작을수로 과적합

min_samples_leaf:말단노드의 최소 샘플데이터수=>비대칭적 데이터는 작게설정



## 2.4 결정트리 시각화

Graphviz

- 결정트리 피쳐중요도

  

- 결정트리 과적합

# 3. 앙상블

> 단일모델의 약점을 다수모델의 결합으로 보완
>
> 부스팅알고리즘들 모두 결정트리알고리즘 기반 알고리즘
>
> 결정트리의 단점인 과적합 down
>
> 장점인 직관적 분류기준 강화

- 보팅과 배깅

  보팅: 서로다른알고리즘가진 분류기

  배깅:  모두 같은 유형 알고리즌기반 but 데이터 샘플링을 다르게 학습수행, 보팅 수행

  ![image-20220203073925045](4. 분류(classification).assets/image-20220203073925045.png)

## 3.1 보팅

![image-20220203074052882](4. 분류(classification).assets/image-20220203074052882.png)

## 3.2 배깅(RandomForest)

랜덤 포레스트: 다재다능 비교적빠른 수행속도, 높은예측성능

![image-20220203074544205](4. 분류(classification).assets/image-20220203074544205.png)

### 부트 스트래핑 분할

중첩되게 샘플링된 데이터 세트 

bootstrap aggregating: Bagging

- 하이퍼 파라미터

  n_extimators = 3 ->3개의 결정트리 기반으로 학습

  ​                                 결정트리의 개수 디폴트는 10개 

  max_features: 디폴트 none 이 아닌 'auto' 즉 'sqrt'

## 3.3 부스팅

여러개의 약한학습기(weak learner) `순차적`으로 학습-예측하면서 **잘못예측한 데이터**에 **가중치부여** 오류개선하며 학습하는 방식

- AdaBoost

- Gradient Boost (GBM)

  가중치 업데이트를 경사하강법을 이용: [y-예측함수] :오류 최소화 방향성갖고 반복적 가중치 업데이트 

  단점: 학습시간 오래걸림, 그리드 서치까지하면 매우오래걸림

- XGBoost

- LightGBM

### 하이퍼 파라미터

- GBM

![image-20220203080010629](4. 분류(classification).assets/image-20220203080010629.png)

![image-20220203081343039](4. 분류(classification).assets/image-20220203081343039.png)

### XGBoost vs LightGBM

xgboost: gbm에 비해 빠른수행 뛰어난예측 자체내장교차검증 결손값자체처리

조기중단/ 규제(l2 람다:기본1 l1 알파:기본0 클수록 규제커짐)

lightgbm: 더빠른 학습 예측수행시간/ 더작은 메모리사용/

​                카테고리형피처(원핫인코딩사용X) 최적변환 ->노드분할(리프중심:오류줄이는방향)

# 4. 이상치와 오버샘플링

## 4.1 이상치 

3/4분위수에서 1.5*IQR더한 지점(최고값) 이상

1/4분위수에서 1.5*IQR 뺀 지점(최고값) 이하

IQR =Q3-Q1

![image-20220203082400425](4. 분류(classification).assets/image-20220203082400425.png)

## 4.2 오버샘플링

레이블이 불균형한 분포를가진경우

![image-20220203082454393](4. 분류(classification).assets/image-20220203082454393.png)

### SMOTE(Synthetic Minority over-sampling Technique)

![image-20220203082607103](4. 분류(classification).assets/image-20220203082607103.png)
