# 1. 피쳐와 타겟
피쳐: 타겟값을 제외한 나머지 속성
타겟/결정   =>        정답 데이터
레이블/클레스 =>분류시 

피쳐 정의
ftr_df = iris_df.iloc[:, :-1]
타겟 정의
tgt_df = iris_df.iloc[:, -1]

# 2. train_test_split()
```from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

dt_clf = DecisionTreeClassifier( )
iris_data = load_iris()

# train_test_split 함수 : 학습, 테스트 데이터 분리
X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, 
                                                    test_size=0.3, random_state=121)
                                                    모델 학습
dt_clf.fit(X_train, y_train)

테스트 데이터로 예측
pred = dt_clf.predict(X_test)
print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))
```




# 3. Kfold
```from sklearn.model_selection import KFold
# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성.
kfold = KFold(n_splits=5)  # n=5
cv_accuracy = []           # 최종적으로는 n번의 교차검증의 평균 정확도 계산
print('붓꽃 데이터 세트 크기:', features.shape[0])
# for문이 도는 동안 generator가 kfold된 데이터의 학습, 검증 row 인덱스를 array로 반환  
kfold.split(features)
n_iter = 0

# KFold객체의 split( ) 호출하면 폴드 별 학습용, 검증용 테스트의 row 인덱스를 array로 반환  
for train_index, test_index  in kfold.split(features):
    # kfold.split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    
    # 학습 및 예측 
    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)
    n_iter += 1
    
    # 반복 시 마다 정확도 측정
    accuracy = np.round(accuracy_score(y_test,pred), 4)  # 정확도 : 소수점 4자리까지 구함
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))
    
    cv_accuracy.append(accuracy)
    
# 개별 iteration별 정확도를 합하여 평균 정확도 계산 
print('\n## 평균 검증 정확도:', np.mean(cv_accuracy)) 
```
## 3.1 stratified K폴드
KFOLD 교차검증의 문제점 : 불균형한 데이터에는 적용이 안된다.
이를 해결할 방법이 StratifiedKFold : 불균형한 분포도를 가진 레이블 데이터 집합을 균형하게 섞어주고 교차검증을 진행한다.

```from sklearn.model_selection import StratifiedKFold
# StratifiedKFold 클래스의 인스턴스 선언 : skf
skf = StratifiedKFold(n_splits=3)
n_iter=0

# StratifiedKFold 사용시 KFold와 차이점 : 레이블 값을 넣어줘서 레이블에 맞게 균일하게 분포를 맞춰준다.
for train_index, test_index in skf.split(iris_df, iris_df['label']):
    n_iter += 1
    label_train= iris_df['label'].iloc[train_index]
    label_test= iris_df['label'].iloc[test_index]
    
    print('## 교차 검증: {0}'.format(n_iter))
    print('학습 레이블 데이터 분포:\n', label_train.value_counts())
    print('검증 레이블 데이터 분포:\n', label_test.value_counts(), '\n')
```

최종점검

```from sklearn.model_selection import StratifiedKFold

dt_clf = DecisionTreeClassifier(random_state=156)

skfold = StratifiedKFold(n_splits=3)
n_iter=0
cv_accuracy=[]

# StratifiedKFold의 split( ) 호출시 반드시 레이블 데이터 셋도 추가 입력 필요  
for train_index, test_index  in skfold.split(features, label):
    # split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = label[train_index], label[test_index]
    
    #학습 및 예측 
    dt_clf.fit(X_train , y_train)    
    pred = dt_clf.predict(X_test)

    # 반복 시 마다 정확도 측정 
    n_iter += 1
    accuracy = np.round(accuracy_score(y_test,pred), 4)
    train_size = X_train.shape[0]
    test_size = X_test.shape[0]
    
    print('\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'
          .format(n_iter, accuracy, train_size, test_size))
    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))
    cv_accuracy.append(accuracy)
    
# 교차 검증별 정확도 및 평균 정확도 계산 
print('\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4))
print('## 평균 검증 정확도:', np.mean(cv_accuracy)) 
```

# 4. 피처 스케일링

## 4.1 정규화와 표준화

#### **정규화와 표준화의 차이는 무엇인가?**

그렇다면 정규화와 표준화의 차이는 무엇일까요? 정규화(normalization)는 다음과 같은 공식을 사용해서 특성 값의 범위를 [0, 1]로 옮깁니다. 

 X′=X−XminXmax−XminX′=X−XminXmax−Xmin  ...(정규화 공식)

  이제야 특성들이 평등한 위치에 놓여진 것입니다. 

 

반면, 표준화(standardization)는 다음과 같은 공식으로 특성들의 값을 변환해줍니다. 

 X′=X−μσX′=X−μσ  ...(표준화 공식)

 즉 종모양의 분포를 따른다고 가정하고 값들을 0의 평균, 1의 표준편차를 갖도록 변환해주는 것입니다. 표준화를 해주면 정규화처럼 특성값의 범위가 0과 1의 범위로 균일하게 바뀌지는 않습니다. 아래 그림을 보시면 정규화와 표준화를 통해 값의 범위가 어떻게 변하는지를 대략적으로 확인하실 수 있습니다. 

![img](https://blog.kakaocdn.net/dn/xaTIu/btqFzbvLruS/KwRY1VUfZIhSKNd6nV97K0/img.jpg)

```from sklearn.preprocessing import StandardScaler
# StandardScaler객체 생성
scaler = StandardScaler()

# StandardScaler 로 데이터 셋 변환. fit( ) 과 transform( ) 호출.  
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)
iris_scaled

# transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환
iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)

```

